[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estadistica No Parametrica",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1*6\n\n[1] 7"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "signo.html#tipos-de-variables-y-escalas-de-medida",
    "href": "signo.html#tipos-de-variables-y-escalas-de-medida",
    "title": "3  Test del Signo",
    "section": "3.1 Tipos de variables y escalas de medida",
    "text": "3.1 Tipos de variables y escalas de medida\n\n3.1.1 Cualitativas\nEscala Nominal: Se presenta cuando los datos no pueden ser ordenados en una secuencia de grados del atributo. p. ej. épocas climáticas (seco, lluvioso), género (hombre, mujer) sitios geográficos (costa, interior llanos, sur), color (rojo, verde, amarillo), marca (renault, mazda, chevrolet), religión (católico, protestante, judaísmo, islam).\nEscala Ordinal: Los datos que pueden ser ordenados de menor a mayor o viceversa, pero las distancias entre los elementos ordenados no tienen ningún sentido físico. Ej: Estrato (I, II,…,VI), Abundancia (abundante, escaso, raro), calidad (buena, mala, regular),talla (grande, mediano, pequeño).\nLos valores ordinales no pueden someterse a operaciones matemáticas como la adición; por eso, no es posible afirmar que el nivel abundante es tres veces mayor que el estado raro.\n\n\n3.1.2 Cuantitativas\nCantidades, que son el resultado de mediciones de algún instrumento, conteos de eventos o de operaciones matemáticas simples. Estos pueden ser\n\nDiscretas: Pueden tomar solo un número finito de posibles valores en la escala de los reales. Ej: número de habitaciones por vivienda, número de hijos por familia, número de peces por estanque, número de temblores por región, número de pacientes por hospital.\nContinuas: Son aquellos en los que existe potencialmente un número infinito de valores entre dos puntos de la escala. Pueden ser datos enteros o fraccionarios, p. ej. Ingreso (pesos), temperatura (grados celsius), peso (kg), tensión arterial (mmHg).\nDerivadas: Son datos generados a partir de cálculos simples de las medidas discretas o continuas, p. ej. índices, tasas, proporciones, etc."
  },
  {
    "objectID": "signo.html#razón-proporción-tasa",
    "href": "signo.html#razón-proporción-tasa",
    "title": "3  Test del Signo",
    "section": "3.2 Razón, Proporción, Tasa",
    "text": "3.2 Razón, Proporción, Tasa\n\nRazón: El numerador no forma parte del denominador. Ej. Número de Personas del Hogar/Número de habitaciones, Número de pacientes/Número de camas, Número de diabéticos/Número de no diabéticos, Número de hombres/Número de mujeres, índice de peso-talla=Kg/(cms-100).\nProporción:El numerador hace parte del denominador. Ej: pacientes recuperados/pacientes tratados, alumnos que aprobaron el examen/ alumnos que presentaron el examen, peso cabeza/peso corporal (no necesariamente conteos!)\nTasa: razón o proporción en la que se define un tiempo de ocurrencia. Ej: a) nacidos vivos periodo /mujeres entre 15 y 50 años en el periodo, b) casos en el periodo/población en riesgo en el periodo."
  },
  {
    "objectID": "signo.html#introducción",
    "href": "signo.html#introducción",
    "title": "3  Test del Signo",
    "section": "3.3 Introducción",
    "text": "3.3 Introducción\nMétodos estadísticos no paramétricos:\n\nTienen supuestos distribucionales más flexibles. Las pruebas clásicas (una muestra, dos muestras, k muestras) asumen normalidad. Hay alternativas no paramétricas libres de este supuesto.\n\nHay técnicas apropiadas para el estudio de variables con escala nominal y ordinal."
  },
  {
    "objectID": "signo.html#mediana",
    "href": "signo.html#mediana",
    "title": "3  Test del Signo",
    "section": "3.4 Mediana",
    "text": "3.4 Mediana\n\nPoblación: Sea \\(X\\) una variable aleatoria con distribución \\(F_x(X)\\). La mediana \\((\\theta)\\) es un valor tal que \\(F_x(\\theta)=\\frac{1}{2}\\) (si \\(X\\) es continua) o \\(P(x\\leq\\theta)\\geq\\frac{1}{2}\\) y \\(P(x\\geq\\theta)\\geq\\frac{1}{2}\\) (si \\(X\\) es discreta).\nSea \\(X\\) ~ exponencial(\\(\\lambda=3\\)). Halle la mediana.\nSea \\(X\\) ~ binomial \\((n = 9; p =\\frac{1}{4})\\). Compruebe que la mediana es 2."
  },
  {
    "objectID": "signo.html#mediana-no-única",
    "href": "signo.html#mediana-no-única",
    "title": "3  Test del Signo",
    "section": "3.5 Mediana No Única",
    "text": "3.5 Mediana No Única\n\nPoblación: Sea \\(X\\) ~ binomial \\((n=21; p=0.5)\\)\n\\(P(X\\leq10)=0.5\\) y \\(P(X\\geq10)=0.67\\). Entonces \\(X=10\\) es una mediana.\n\\(P(X\\leq11)=0.67\\) y \\(P(X\\geq11)=0.5\\). Entonces \\(X=11\\) es una mediana.\n\\(X\\) ~ binomial \\((n = 21; p =0.5)\\) la mediana no es única.\n\n\n3.5.1 Test del Signo\nPoblación: Sea \\(X\\) una variable aleatoria cuya distribución tiene mediana \\(\\theta\\), es decir \\(\\theta\\) es un valor tal que \\(F_x(\\theta)=\\frac{1}{2}\\) Muestra: \\(X_1\\); \\(X_2\\);…; \\(X_n\\) una muestra aleatoria de \\(X\\).\nHipótesis:\n\\[\\begin{aligned} H_{0}: \\theta & =\\theta_{0} \\\\ H_{a}: \\theta & \\neq \\theta_{0} \\\\ \\theta & &gt;\\theta_{0} \\\\ \\theta & &lt;\\theta_{0}\\end{aligned}\\] Sea \\(Y_1=X_1-\\theta_0...;Y_n=X_n-\\theta_0\\). Entonces las hipótesis pueden plantearse como\n\\[\\begin{aligned} H_{0}: \\theta & =0 \\\\ H_{a}: \\theta & \\neq 0 \\\\ \\theta & &gt;0 \\\\ \\theta & &lt;0\\end{aligned}\\] Es claro que \\(\\theta\\) representa la mediana de \\(Y\\).\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}1 & \\text { if } Y_{i}&gt;0 \\\\ 0 & \\text { if } Y_{i}&lt;0\\end{array}\\right.\\]\nBajo \\(H_{0}, P\\left(\\psi_{i}=1\\right)=P\\left(Y_{i}&gt;0\\right)=\\frac{1}{2}\\)\n\\[S=\\sum_{i=1}^{n} \\psi_{i} \\sim \\operatorname{Bin}\\left(n, \\frac{1}{2}\\right)\\]\n\nEstadística de prueba y región de rechazo. \\(H_a:\\theta&gt;0\\)\n\nSe rechaza \\(H_0\\) si \\(S \\geq b_{(1-\\alpha, 1 / 2)}\\) donde \\(b_{(1-\\alpha, 1 / 2)}\\) es el percentil \\(1-\\alpha\\) de la distribución binomial \\(n,p=\\frac{1}{2}\\).\n\n# Una cola superior: Se rechaza la nivel alpha=6\\% si Sc&gt;11\nn=15\np=.5\nx=seq(0, n, by=1)\ncbind(x,round(pbinom(x-1, n, p, lower.tail=FALSE) ,2))\n\n       x     \n [1,]  0 1.00\n [2,]  1 1.00\n [3,]  2 1.00\n [4,]  3 1.00\n [5,]  4 0.98\n [6,]  5 0.94\n [7,]  6 0.85\n [8,]  7 0.70\n [9,]  8 0.50\n[10,]  9 0.30\n[11,] 10 0.15\n[12,] 11 0.06\n[13,] 12 0.02\n[14,] 13 0.00\n[15,] 14 0.00\n[16,] 15 0.00\n\n\n\nEstadística de prueba y región de rechazo. \\(H_a:\\theta&lt;0\\)\n\nSe rechaza \\(H_0\\) si \\(S \\leq n-b_{(1-\\alpha, 1 / 2)}\\), donde \\(b_{(1-\\alpha, 1 / 2)}\\) es el percentil \\(1-\\alpha\\) de la distribución binomial \\(n,p=\\frac{1}{2}\\)\n\n# Una cola inferior: Se rechaza al nivel alpha=6\\% si Sc&lt;=4\nn=15\np=.5\nx=seq(0, n, by=1)\ncbind(x,round(pbinom(x, n, p) ,2))\n\n       x     \n [1,]  0 0.00\n [2,]  1 0.00\n [3,]  2 0.00\n [4,]  3 0.02\n [5,]  4 0.06\n [6,]  5 0.15\n [7,]  6 0.30\n [8,]  7 0.50\n [9,]  8 0.70\n[10,]  9 0.85\n[11,] 10 0.94\n[12,] 11 0.98\n[13,] 12 1.00\n[14,] 13 1.00\n[15,] 14 1.00\n[16,] 15 1.00\n\n\n\nEstadística de prueba y región de rechazo. \\(H_a:\\theta \\neq 0\\)\n\nSe rechaza \\(H_0\\) si \\(S \\leq n-b_{(1-\\alpha /2, 1 / 2)}\\) o \\(S \\geq b_{(1-\\alpha/2, 1 / 2)}\\), donde \\(b_{(1-\\alpha/2, 1 / 2)}\\) es el percentil \\(1-\\alpha/2\\) de la distribución binomial \\(n,p=\\frac{1}{2}\\).\n\n# Dos colas: Se rechaza la nivel alpha=12\\% si Sc&gt;=11 o Sc&lt;=4 \nn=15\np=.5\nx=seq(0, n, by=1)\ncbind(x,round(pbinom(x, n, p) ,2), round(pbinom(x-1, n, p, lower.tail=FALSE) ,2))\n\n       x          \n [1,]  0 0.00 1.00\n [2,]  1 0.00 1.00\n [3,]  2 0.00 1.00\n [4,]  3 0.02 1.00\n [5,]  4 0.06 0.98\n [6,]  5 0.15 0.94\n [7,]  6 0.30 0.85\n [8,]  7 0.50 0.70\n [9,]  8 0.70 0.50\n[10,]  9 0.85 0.30\n[11,] 10 0.94 0.15\n[12,] 11 0.98 0.06\n[13,] 12 1.00 0.02\n[14,] 13 1.00 0.00\n[15,] 14 1.00 0.00\n[16,] 15 1.00 0.00\n\n\n\n# alpha=10% un cola inferior\n# Se rechaza $H_0$ si $S\\geq b_{(\\alpha, 1/2)}$, \n# donde $b_{(\\alpha, 1/2)}$ es el percentil $\\alpha$ \n# de la distribuci?n binomial $(n, p=1/2)$"
  },
  {
    "objectID": "signo.html#resultados-asintóticos",
    "href": "signo.html#resultados-asintóticos",
    "title": "3  Test del Signo",
    "section": "3.6 Resultados asintóticos",
    "text": "3.6 Resultados asintóticos\n\n\\(X_1,...X_n\\) m.a de \\(X\\) con distribución \\(P(X \\leq x)\\)\nAsintóticamente \\(\\sum_{i=1}^{n} X_{i} \\sim N\\left(E\\left(\\sum_{i=1}^{n} X_{i}\\right), V\\left(\\sum_{i=1}^{n} X_{i}\\right)\\right)\\)\nAsintóticamente \\(Z=\\left(\\frac{\\sum_{i=1}^{n} x_{i}-E\\left(\\sum_{i=1}^{n} x_{i}\\right)}{\\sqrt{V\\left(\\sum_{i=1}^{n} x_{i}\\right)}}\\right) \\sim N(0,1)\\)\nAsintóticamente \\(\\bar{X} \\sim N(\\mathbb{E}(\\bar{X}), \\mathbb{V}(\\bar{X}))\\)\nAsintóticamente \\(Z=\\left(\\frac{\\bar{X}-\\mathbb{E}(\\bar{X})}{\\sqrt{\\mathbb{V}(\\bar{X})}}\\right) \\sim N(0,1)\\)\n\\(\\psi_{1},...\\psi_{n}\\) m.a. con \\(\\psi_{i}\\) ~ Bernoulli (1/2)\nAsintóticamente \\(S=\\sum_{i=1}^{n} \\psi_{i} \\sim N\\left(\\frac{n}{2}, \\frac{n}{4}\\right)\\)\nAsintóticamente \\(Z=\\left(\\frac{S-\\left(\\frac{n}{2}\\right)}{\\sqrt{\\left(\\frac{n}{4}\\right)}}\\right) \\sim N(0,1)\\)\nAsintóticamente \\(\\frac{S}{n} \\sim N\\left(\\left(\\frac{1}{2}\\right),\\left(\\frac{1}{4 n}\\right)\\right)\\)\nAsintóticamente \\(Z=\\left(\\frac{\\frac{S}{n}-\\left(\\frac{1}{2}\\right)}{\\sqrt{\\left(\\frac{1}{4 n}\\right)}}\\right) \\sim N(0,1)\\)"
  },
  {
    "objectID": "signo.html#test-del-signo-1",
    "href": "signo.html#test-del-signo-1",
    "title": "3  Test del Signo",
    "section": "3.7 Test del signo",
    "text": "3.7 Test del signo\nUsando la Distribución Asintóticapara el caso de \\(H_a:\\theta&gt;0\\). Encontrar, empleando la distribución asintótica, el valor de \\(k\\) tal que \\(P(S\\geq k)=\\alpha\\)\n\\[Z=\\frac{S-\\frac{n}{2}}{\\sqrt{\\frac{n}{4}}} \\sim N(0,1)\\] \\[\\begin{aligned} P(S \\geq k) & =P\\left(\\frac{S-\\frac{n}{2}}{\\sqrt{\\frac{n}{4}}}&gt;\\frac{k-\\frac{n}{2}}{\\sqrt{\\frac{n}{4}}}\\right) \\\\ & =P\\left(Z&gt;\\frac{k-\\frac{n}{2}}{\\sqrt{\\frac{n}{4}}}\\right) \\\\ & =P\\left(Z&gt;Z_{1-\\alpha}\\right) \\\\ k & =\\frac{n}{2}+Z_{1-\\alpha} \\sqrt{\\frac{n}{4}}\\end{aligned}\\] Ejemplo:El porcentaje de bacterias en muestras de agua de desecho no debe ser superior al 40%. La observación de 10 “muestras” de agua de desecho arrojó los siguientes porcentajes:\n\\[\\begin{array}{r}\nx_{i}=41,33,43,52,46,37,44,49,53,30 . \\\\\nH_{0}: \\theta=40 \\\\\nH_{a}: \\theta&gt;40\n\\end{array}\\]\nSe obtienen los datos:\n\\[\\begin{array}{l}\ny_{i}=\\left(x_{i}-40\\right)=1,-7,3,12,3,-3,4,9,13,-10 . \\\\\n\\psi_{i}=1,0,1,1,1,0,1,1,1,0 . \\\\\nS_{c}=\\sum_{i=1}^{n} \\psi_{i}=7 . \\quad P(S \\geq 7)=0.172 \\quad S \\sim \\operatorname{Bin}(10,1 / 2) .\n\\end{array}\\]\nsi \\(\\alpha=5 \\%\\) entonces se rechaza \\(H_0\\)"
  },
  {
    "objectID": "signo.html#test-de-wilcoxon",
    "href": "signo.html#test-de-wilcoxon",
    "title": "3  Test del Signo",
    "section": "3.8 Test de Wilcoxon",
    "text": "3.8 Test de Wilcoxon\nPoblación: \\(X\\) una variable aleatoria con distribución simétrica y continua de mediana \\(\\theta\\) (Hollander, M. and Wolfe, D. 1999. Nonparametric Statistical Methods. John Wiley & Sons)\nDistribuciones simétricas continuas: Weibull, Gamma, Beta, Uniforme, Triangular, etc.\nMuestra: \\(X_1, X_2,...X_n\\) una muestra aleatoria de \\(X\\).\nHipótesis\n\\[\\begin{aligned} H_{0}: & =\\theta_{0} \\\\ H_{a}: \\theta & \\neq \\theta_{0} \\\\ \\theta & &gt;\\theta_{0} \\\\ \\theta & &lt;\\theta_{0}\\end{aligned}\\] \\(Y_{1}=X_{1}-\\theta_{0}, Y_{2}=X_{2}-\\theta_{0}, \\ldots, Y_{n}=X_{n}-\\theta_{0}\\) una muestra aleatoria de \\(X\\)\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i}&gt;0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\n\\(R_{i}^{+}=\\) posición que ocupa \\(\\left|Y_{i}\\right|\\) en la sucesión ordenada \\[|Y|_{(1)} \\leq|Y|_{(2)} \\leq \\cdots \\leq|Y|_{(n)}\\] Estadística de prueba:\n\\[T=\\sum_{i=1}^{n} R_{i}^{+} \\psi_{i}\\] ver Tabla A.4 Hollander & Wolfe para valores críticos. Se puede demostrar (ejercicio) que cuando \\(n\\) es grande\n\\[\\begin{array}{c}\nE(T)=\\frac{n(n+1)}{4} \\\\\nV(T)=\\frac{n(n+1)(2 n+1)}{24}\n\\end{array}\\]\nDistribución Asintótica: Para \\(n\\) grande\n\\[Z=\\frac{T-E(T)}{\\sqrt{V(T)}} \\sim N(0,1)\\]\nRegión de rechazo\nSe rechaza \\(H_0\\) si \\(T\\geq k\\), con \\(k\\) tal que \\(P(T\\geq k) =\\alpha\\)\n\\[k=\\frac{n(n+1)}{4}+Z_{1-\\alpha} \\sqrt{\\frac{n(n+1)(2 n+1)}{24}}\\] Ejemplo: En un grupo de 12 personas se mide el cambio en el ritmo cardíaco (latidos/minuto) después de levantarse. \\(H_0 : \\theta = 15\\).\nPEGAR TABLA DE DIAPOSITIVA 25\n\\[\\begin{aligned}\nT_{c} & =\\sum R_{i}^{+} S\\left(y_{i}\\right)=14 \\\\\nE(T) & =\\frac{n(n+1)}{4}=39 \\\\\nV(T) & =\\frac{n(n+1)(2 n+1)}{24}=162.5 \\\\\nZ_{c} & =\\frac{T_{c}-E(T)}{\\sqrt{V(T)}}=-1.9611\n\\end{aligned}\\]\nSe rechaza \\(H_{0}\\) al 5%, si \\(Z_{c}&gt;Z_{1-\\alpha}=1.645\\) Luego, no hay evidencia para rechazar \\(H_{0}\\).\n\nDistribución exacta: Se construye generando el conjunto de todos los arreglos posibles (formas como pueden aparecer observaciones positivas y negativas en los en la muestra y obteniendo todos los posibles valores de T.\nValores críticos: Ver tablas en (Hollander, M. and Wolfe, D. 1999. Nonparametric Statistical Methods. John Wiley & Sons)\nCeros: Si hay ceros en los \\(Y_i\\), se descartan y se toma \\(n\\) como el número de valores distintos de cero.\nEmpates: Si hay \\(|Y_i|, R_i^+\\) empates en los corresponde al promedio de los rangos correspondientes.\n\nEjemplo cuando hay empates: En un grupo de 12 personas se mide el cambio en el ritmo cardíaco (latidos/minuto) después de levantarse. \\(H_0 : \\theta = 15\\).\nPEGAR TABLA DE DIAPOSITIVA 29\n\\[\\begin{aligned}\nT_{c} & =\\sum R_{i}^{+} S\\left(y_{i}\\right)=14.5 \\\\\nE(T) & =\\frac{n(n+1)}{4}=39 \\\\\nV(T) & =\\frac{n(n+1)(2 n+1)}{24}=162.375 \\\\\nZ_{c} & =\\frac{T_{c}-E(T)}{\\sqrt{V(T)}}=-1.9226\n\\end{aligned}\\]\nSe rechaza \\(H_0\\) al 5% si \\(Z_{c}&gt;Z_{1-\\alpha}=1.645\\) Luego, no hay evidencia para rechazar \\(H_0\\)"
  },
  {
    "objectID": "pareadas.html#ejemplo-conteo-de-bacterias-en-muestras-de-leche",
    "href": "pareadas.html#ejemplo-conteo-de-bacterias-en-muestras-de-leche",
    "title": "4  Muestras pareadas",
    "section": "4.1 Ejemplo: Conteo de Bacterias en muestras de leche",
    "text": "4.1 Ejemplo: Conteo de Bacterias en muestras de leche\nDiez muestras de leche se dividen en dos porciones. Una porción se envía al laboratorio I y la otra al laboratorio II. Cada laboratorio hace recuentos bacterianos (miles de bacterias por ml). Los datos se muestran en la tabla a continuación. Con base en los datos puede concluirse que hay diferencias entre los laboratorios?. Puede emplearse una prueba t-student para muestras pareadas?.\nEjemplo: 10 muestras de leche. A cada una se le mide el contenido bacteriano (miles de bacterias por ml) en dos laboratorios. Hay diferencias entre los laboratorios?\nPEGAR TABLA DE DIAPOSITIVA 5"
  },
  {
    "objectID": "pareadas.html#prueba-t-pareada.-hipótesis",
    "href": "pareadas.html#prueba-t-pareada.-hipótesis",
    "title": "4  Muestras pareadas",
    "section": "4.2 Prueba T pareada. Hipótesis",
    "text": "4.2 Prueba T pareada. Hipótesis\n\nMuestra aleatoria pareada:\n\\(\\left(X_{1_{1}}, X_{2_{1}}\\right), \\ldots,\\left(X_{1_{n}}, X_{2_{n}}\\right) . D_{i}=\\left(X_{1_{i}}-X_{2_{i}}\\right), i=1, \\ldots, n.\\)\nMuestra aleatoria pareada observada:\n\\(\\left(x_{1_{1}}, x_{2_{1}}\\right), \\ldots,\\left(x_{1_{n}}, x_{2_{n}}\\right) \\cdot d_{i}=\\left(x_{1_{i}}-x_{2_{i}}\\right), i=1, \\ldots, n \\text {. }\\)\nMuestra aleatoria: \\(D_1,...D_n\\)\nHipótesis pueden plantearse como:\n\\[\\begin{array}{l}\nH_{0}: \\mu_{D}=0 \\\\\nH_{a}: \\mu_{D} \\neq 0 \\\\\n\\mu_{D}&gt;0 \\\\\n\\mu_{D}&lt;0\n\\end{array}\\]"
  },
  {
    "objectID": "pareadas.html#estadística-de-prueba-y-región-de-rechazo",
    "href": "pareadas.html#estadística-de-prueba-y-región-de-rechazo",
    "title": "4  Muestras pareadas",
    "section": "4.3 Estadística de Prueba y Región de Rechazo",
    "text": "4.3 Estadística de Prueba y Región de Rechazo\n\\[\\begin{aligned}\nT & =\\frac{\\bar{D}}{\\frac{S_{D}}{\\sqrt{n}}} \\sim \\text { t-student }_{n-1}, \\text { con } \\\\\n\\bar{D} & =\\frac{\\sum_{i=1}^{n} D_{i}}{n} \\text { y } S_{D}=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(D_{i}-\\bar{D}\\right)^{2}}{n-1} .} \\\\\n\\text { Sea } T_{c} & =\\frac{\\bar{d}}{\\frac{s_{d}}{n}} \\operatorname{con} \\bar{d}=\\frac{\\sum_{i=1}^{n} d_{i}}{n} \\text { y } s_{d}=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(d_{i}-\\bar{d}\\right)^{2}}{n-1}}\n\\end{aligned}\\]\nSe rechaza la hipótesis nula al nivel \\(\\alpha\\) si\n\\[\\begin{array}{l}\nT_{c}&lt;t_{\\frac{\\alpha}{2}} \\circ T_{c}&gt;t_{1-\\frac{\\alpha}{2}} \\\\\nT_{c}&lt;t_{\\alpha} \\\\\nT_{c}&gt;t_{1-\\alpha}\n\\end{array}\\]"
  },
  {
    "objectID": "pareadas.html#test-del-signo.-muestras-pareadas",
    "href": "pareadas.html#test-del-signo.-muestras-pareadas",
    "title": "4  Muestras pareadas",
    "section": "4.4 Test del signo. Muestras pareadas",
    "text": "4.4 Test del signo. Muestras pareadas\n\nPoblación: sea \\((X_1-X_2)\\) una variable aleatoria bivariada y \\(Y=\\left(X_{1}-X_{2}\\right)\\). Suponga que \\(Y\\) tiene mediada \\(\\theta, F_{Y}(\\theta)=\\frac{1}{2}\\) (si \\(Y\\) es continua) o \\(\\circ P(Y \\leq \\theta) \\geq \\frac{1}{2}\\) y \\(P(Y \\geq \\theta) \\geq \\frac{1}{2}\\) (si \\(Y\\) es discreta).\nMuestra: \\(Y_{1}, Y_{2}, \\ldots, Y_{n}\\) una muestra aleatoria de \\(Y\\).\nMuestra observada \\(Y_{1}, Y_{2}, \\ldots, Y_{n}\\) una muestra aleatoria observada de \\(Y\\).\nHIpótesis:\n\n\\[\\begin{aligned}\nH_{0}: \\theta & =0 \\\\\nH_{a}: \\theta & \\neq 0 \\\\\n\\theta & &gt;0 \\\\\n\\theta & &lt;0\n\\end{aligned}\\]\n\nEstadística de prueba y región de rechazo. \\(H_{a}: \\theta&gt;0\\)\n\n\\[S=\\sum_{i=1}^{n} \\psi_{i}\\] con,\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i}&gt;0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\nBajo \\(H_{0}, P\\left(\\psi_{i}=1\\right)=P\\left(Y_{i}&gt;0\\right)=\\frac{1}{2}\\) \\[\\Rightarrow S \\sim \\operatorname{Bin}\\left(n, \\frac{1}{2}\\right)\\] Se rechaza \\(H_0\\) si\n\\(S \\geq k\\), donde \\(k\\) es tal que \\(P(S \\geq k)=\\alpha\\), con \\(\\alpha\\) el nivel de significancia.\n\nEstadística de prueba y región de rechazo. \\(H_{a}: \\theta&lt;0\\)\n\n\\[S=\\sum_{i=1}^{n} \\psi_{i}\\] con,\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i}&gt;0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\nBajo \\(H_{0}, P\\left(Y_{i}=1\\right)=P\\left(Y_{i}&gt;0\\right)=\\frac{1}{2}\\)\n\\[\\Rightarrow S \\sim \\operatorname{Bin}\\left(n, \\frac{1}{2}\\right)\\] Se rechaza \\(H_0\\) si\n\\(S \\leq k\\), donde \\(k\\) es tal que \\(P(S \\leq k)=\\alpha\\) y \\(alpha\\) es el nivel de significancia.\n\nEstadística de prueba y región de rechazo. \\(H_{a}: \\theta \\neq 0\\)\n\n\\[S=\\sum_{i=1}^{n} \\psi_{i}\\]\ncon,\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i} \\geq 0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\nBajo \\(H_{0}, P\\left(Y_{i}=1\\right)=P\\left(Y_{i}&gt;0\\right)=\\frac{1}{2}\\)\n\\[\\Rightarrow S \\sim \\operatorname{Bin}\\left(n, \\frac{1}{2}\\right)\\]\nSe rechaza \\(H_0\\) si\n\\(S \\leq k_{1}\\), donde \\(k_{1}\\) es tal que \\(P\\left(S \\leq k_{1}\\right)=\\alpha / 2\\), o si \\(S \\geq k_{2}\\), donde \\(k_{2}\\) es tal que \\(P\\left(S \\geq k_{2}\\right)=\\alpha / 2\\), con \\(\\alpha\\) el nivel de significancia."
  },
  {
    "objectID": "pareadas.html#test-de-wilcoxon.-muestras-pareadas",
    "href": "pareadas.html#test-de-wilcoxon.-muestras-pareadas",
    "title": "4  Muestras pareadas",
    "section": "4.5 Test de Wilcoxon. Muestras pareadas",
    "text": "4.5 Test de Wilcoxon. Muestras pareadas\n\nPoblación: \\(\\left(X_{1}, X_{2}\\right)\\) variable aleatoria bivariada. \\(Y=\\left(X_{1}-X_{2}\\right)\\). Suponga que \\(Y\\) tiene distribución continua simétrica con mediana \\(\\theta\\).\nMuestra \\(Y_{1}, \\ldots, Y_{n}\\) una muestra aleatoria de \\(Y\\).\nHipótesis:\n\n\\[\\begin{aligned}\nH_{0}: \\theta & =0 \\\\\nH_{a}: \\theta & \\neq 0 \\\\\n\\theta & &gt;0 \\\\\n\\theta & &lt;0\n\\end{aligned}\\]\n\\(Y_{1}, Y_{2}, \\ldots, Y_{n}\\) una muestra aleatoria de \\(Y\\).\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i} \\geq 0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\n\\(R_{i}^{+}=\\) Posición que ocupa \\(\\left|Y_{i}\\right|\\) en la sucesión ordenada\n\\[|Y|_{(1)} \\leq|Y|_{(2)} \\leq \\ldots \\leq|Y|_{(n)}\\]\n\nEstadística de prueba:\n\n\\[\\begin{array}{l}\nT=\\sum_{i=1}^{n} R_{i}^{+} \\psi_{i} \\\\\nT_{c}=\\sum_{i=1}^{n} R_{i}^{+} \\psi_{i}\n\\end{array}\\]\nSe puede demostrar que\n\\[\\begin{array}{c}\nE(T)=\\frac{n(n+1)}{4} \\\\\nV(T)=\\frac{n(n+1)(2 n+1)}{24}\n\\end{array}\\]\n\nDistribución asintótica: Para \\(n\\) grande.\n\n\\[Z=\\frac{T-E(T)}{\\sqrt{V(T)}} \\sim N(0,1)\\] El valor calculado de la estadística de prueba es\n\\[Z_{c}=\\frac{T_{c}-E(T)}{\\sqrt{V(T)}}\\]\nRegión de rechazo. \\(H_a:\\theta&gt;0\\) Se rechaza \\(H_0\\) si \\(Z_{c}&gt;z_{1-\\alpha}\\).\nRegión de rechazo. \\(H_a:\\theta&lt;0\\) Se rechaza \\(H_0\\) si \\(Z_{c}&lt;z_{\\alpha}\\)\nRegión de rechazo. \\(H_a:\\theta \\neq 0\\) Se rechaza \\(H_0\\) si \\(Z_{c}&lt;z_{\\frac{\\alpha}{2}} \\circ Z_{c}&gt;z_{1-\\frac{\\alpha}{2}}\\)\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "muestrasind.html#prueba-clásica-asumiendo-normalidad-prueba-t-de-dos-muestras-independientes",
    "href": "muestrasind.html#prueba-clásica-asumiendo-normalidad-prueba-t-de-dos-muestras-independientes",
    "title": "5  Muestras independientes",
    "section": "5.1 Prueba clásica asumiendo normalidad Prueba T de dos muestras independientes",
    "text": "5.1 Prueba clásica asumiendo normalidad Prueba T de dos muestras independientes\n\nMuestra aleatoria \\(X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m}\\)\nMuestra aleatoria observada \\(x_{1}, \\ldots, x_{n}, y_{1}, \\ldots, y_{m}\\)\nHipótesis\n\n\\[\\begin{aligned}\nH_{0}: \\mu_{1} & =\\mu_{2} \\\\\nH_{a}: \\mu_{1} & \\neq \\mu_{2} \\\\\n\\mu_{1} & &gt;\\mu_{2} \\\\\n\\mu_{1} & &lt;\\mu_{2}\n\\end{aligned}\\]\n\n5.1.1 Estadística de prueba y región de rechazo\n\\[\\begin{aligned}\nT & =\\frac{(\\bar{X}-\\bar{Y})}{S_{p} \\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\sim \\text { t-student }{ }_{n+m-2}, \\text { con } \\\\\nS_{p} & =\\sqrt{\\frac{(n-1) S_{x}^{2}+(m-1) S_{y}^{2}}{n+m-2}} . \\\\\nS_{\\text {Sea }} T_{c} & =\\frac{(\\bar{x}-\\bar{y})}{s_{p} \\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\operatorname{con} s_{p}=\\sqrt{\\frac{(n-1) s_{x}^{2}+(m-1) s_{y}^{2}}{n+m-2}}\n\\end{aligned}\\]\nSe rechaza la hipótesis nula al nivel \\(\\alpha\\) si\n\\[\\begin{array}{l}\nT_{c}&lt;t_{\\frac{\\alpha}{2}} \\circ T_{c}&gt;t_{1-\\frac{\\alpha}{2}} \\\\\nT_{c}&gt;t_{1-\\alpha} \\\\\nT_{c}&lt;t_{\\alpha}\n\\end{array}\\]"
  },
  {
    "objectID": "muestrasind.html#pruebas-no-paramétricas-para-dos-muestras-independientes",
    "href": "muestrasind.html#pruebas-no-paramétricas-para-dos-muestras-independientes",
    "title": "5  Muestras independientes",
    "section": "5.2 Pruebas no paramétricas para dos muestras independientes",
    "text": "5.2 Pruebas no paramétricas para dos muestras independientes\n\nLos datos provienen de distribuciones continuas\nSe asume independencia (dentro de muestras y entre muestras)\nLas pruebas de Wilcoxon y de Mann-Whitney para dos muestras son alternativas no paramétricas a la prueba T-student para dos muestras."
  },
  {
    "objectID": "muestrasind.html#pruebas-de-wilcoxon-y-de-mann-whitney",
    "href": "muestrasind.html#pruebas-de-wilcoxon-y-de-mann-whitney",
    "title": "5  Muestras independientes",
    "section": "5.3 Pruebas de Wilcoxon y de Mann-Whitney",
    "text": "5.3 Pruebas de Wilcoxon y de Mann-Whitney\n\nDos versiones del test, descritas comoU de Mann–Whitney y W de Wilcoxon, fueron independientemente desarrolladas por Mann y Whitney (1947) y Wilcoxon (1949).\nLa versión por Wilcoxon (1949) es usualmente llamada test W de Wilcoxon–Mann-Whitney.\nAunque se emplean diferentes ecuaciones y diferentes tablas de distribuciones exactas, las dos versiones producen resultados comparables.\n\n\n5.3.1 Ejemplo dieta de cabras\n\nTomado de McFarland, T and Yates, J. 2016. Introduction to non parametric statistics for the biological sciences using R. Springer. Una manada de 30 cabras fue dividida en dos grupos. La asignación al grupo 1 o al grupo 2 se basó en una selección aleatoria.\nGrupo 1: Control. Estas cabras recibieron alimentación regular durante el tratamiento.\nGrupo 2: Tratamiento. Estas cabras recibieron un suplemento mineral además de la alimentación regular. Respuesta: Un valor (índice) entre 40 y 100 dependiendo de las condiciones de la cabra.\nEjemplo: 15 cabras asignadas a un grupo control (dieta regular) y uno un grupo de tratamiento (dieta regular con adición de minerales). La respuesta es un índice con valores entre 40 y 100.\nPEGAR TABLA DE DIAPOSITIVA 8\n5.4 Test de Wilcoxon\n\n\n\n5.4.1 Supuesto poblacional e hipótesis\n\n\\(X_{1} \\ldots, X_{m}\\) muestra aleatoria de \\(X \\sim F_{X}(t)\\) continua\n\\(Y_{1} \\ldots, Y_{n}\\) muestra aleatoria de \\(Y \\sim F_{Y}(t)\\) continua\nIndependencia entre \\(X\\) y \\(Y\\)\nHipótesis\n\n\\[\\begin{array}{l}\nH_{0}: F_{Y}(t)=F_{X}(t) \\text { para todo } t \\\\\nH_{a}: F_{Y}(t)=F_{X}(t-\\Delta) \\text { para todo } t \\text { y algún } \\Delta \\neq 0\n\\end{array}\\]\nGibbons, J and Chakraborti, S. 2003. Nonparametrical Statistical Inference. Marcel Dekker.\n\n\n5.4.2 Normal\nPEGAR FIGURA DIAPOSITIVA 10\n\n\n5.4.3 Exponencial y exponencial desplazada\nPEGAR FIGURA DIAPOSITIVA 11\n\n\n5.4.4 Weibull y Weibull Desplazada\nPEGAR FIGURA DIAPOSITIVA 12\n∆ en (1) se llama parámetro de desplazo o efecto de tratamiento.\n\nLas hipótesis pueden plantearse como\n\n\\[\\begin{aligned}\nH_{0}: \\Delta & =0 \\\\\nH_{a}: \\Delta & \\neq 0 \\\\\n\\Delta & &gt;0 \\\\\n\\Delta & &lt;0\n\\end{aligned}\\]\n\nMuestra Combinada: sea \\(X_{1}, X_{2}, \\ldots, X_{m}, X_{m+1}, \\ldots, X_{N}\\), \\(N=m+n\\) la muestra combinada. \\(X^{\\prime} s\\) \\(X_{m+1}, \\ldots, X_{N}\\) las observaciones de las \\(Y^{\\prime} s\\).\n\n\n\n5.4.5 Estadística de prueba\n\nSe ordena la muestra combinada de \\(N=m+n\\) variables.\nLlámese \\(W=\\) suma de los rangos de las \\(Y^{\\prime} s\\) en la muestra combinada. Sean \\(S_{1}, S_{2}, \\ldots, S_{n}\\). (W puede definirse como la suma de los rangos de las \\(X^{\\prime} s\\))\n\n\\[\\begin{aligned}\nW & =\\sum_{i=1}^{n} S_{j} \\\\\n\\mathbb{E}(W) & =\\frac{n(N+1)}{2}=\\frac{n(m+n+1)}{2} \\\\\n\\mathbb{V}(W) & =\\frac{n m(N+1)}{12}=\\frac{n m(m+n+1)}{12}\n\\end{aligned}\\]\n\n\n5.4.6 Distribución asintótica\n\\[Z=\\frac{W-\\mathbb{E}(W)}{\\sqrt{\\mathbb{V}(W)}} \\sim N(0,1)\\]\nSe rechaza la hipótesis nula \\(\\alpha\\) si\n\\[\\begin{array}{l}\nZ_{c}&lt;z_{\\frac{\\alpha}{2}} \\circ Z_{c}&gt;z_{1-\\frac{\\alpha}{2}} \\\\\nZ_{c}&gt;z_{1-\\alpha} \\\\\nZ_{c}&lt;z_{\\alpha}\n\\end{array}\\]\n\n\n5.4.7 Distribución exacta\nSuponga \\(m =3\\) valores de \\(X\\) y \\(n = 2\\) valores de \\(Y\\). Los rangos van entre 1 y 5. Ver Hollander and Wolfe página 108.\nPEGAR TABLA DIAPOSITIVA 16\n\n\n5.4.8 Empates\n\nCaso de distribución exacta: Se da a las observaciones empatadas el promedio de los correspondientes rangos, se calcula W y se usa la misma tabla\nSi se emplea la distribución asintótica cambia la varianza\n\n\\[V(W)=\\frac{m n(N+1)}{12}-\\left(\\frac{m n}{12 N(N-1)} \\sum_{j=1}^{g}\\left(t_{j}-1\\right) t_{j}\\left(t_{j}+1\\right)\\right)\\]\ncon \\(g\\) el número de grupos con empates y \\(t_{j}\\) el número de observaciones empatadas en el j-ésimo grupo."
  },
  {
    "objectID": "muestrasind.html#test-de-mann-whitney",
    "href": "muestrasind.html#test-de-mann-whitney",
    "title": "5  Muestras independientes",
    "section": "5.5 Test de Mann-Whitney",
    "text": "5.5 Test de Mann-Whitney\n\n5.5.1 Hipótesis\nComo en el test de Wilcoxon, las hip´otesis pueden plantearse como\n\\[\\begin{aligned} H_{0}: \\Delta & =0 \\\\ H_{a}: \\Delta & \\neq 0 \\\\ \\Delta & &gt;0 \\\\ \\Delta & &lt;0\\end{aligned}\\]\n\n\n5.5.2 Estadística de prueba\nSea\n\\[\\phi(a)=\\left\\{\\begin{array}{ll}\n1 & \\text { if } a&gt;0 \\\\\n0 & \\text { if } a&lt;0\n\\end{array}\\right.\\]\nLa estadística de Mann-Whitney se define como:\n\\[\\begin{aligned}\nU & =\\sum_{i=1}^{n} \\sum_{j=1}^{m} \\phi\\left(Y_{i}-X_{j}\\right) \\\\\n& =\\#\\left(Y_{i}-X_{j}\\right)&gt;0\n\\end{aligned}\\]\nSi no hay empates\n\\[U=W-\\frac{n(n+1)}{2}\\]\n\n\n5.5.3 Distribución asintótica\n\\[\\begin{aligned} U & =W-\\frac{n(n+1)}{2} \\\\ \\mathbb{E}(U) & =\\mathbb{E}(W)-\\frac{n(n+1)}{2} \\\\ & =\\frac{n(N+1)}{2}-\\frac{n(n+1)}{2} \\\\ & =\\frac{n(N+1)-n(n+1)}{2} \\\\ & =\\frac{n(N+1-(n+1))}{2} \\\\ & =\\frac{n(n+m+1)-(n+1)}{2} \\\\ & =\\frac{n m}{2}\\end{aligned}\\]\n\\[\\begin{aligned}\nU & =W-\\frac{n(n+1)}{2} \\\\\n\\mathbb{V}(U) & =\\mathbb{V}(W)-\\mathbb{V}\\left(\\frac{n(n+1)}{2}\\right) \\\\\n& =\\frac{n m(m+n+1)}{12} \\\\\nZ & =\\frac{U-\\mathbb{E}(U)}{\\sqrt{\\mathbb{V}(U)}} \\sim N(0,1)\n\\end{aligned}\\]\nSe rechaza la hipótesis nula al nivel \\(\\alpha\\) si\n\\[\\begin{array}{l}\nZ_{c}&lt;z_{\\frac{\\alpha}{2}} \\circ Z_{c}&gt;z_{1-\\frac{\\alpha}{2}} \\\\\nZ_{c}&gt;z_{1-\\alpha} \\\\\nZ_{c}&lt;z_{\\alpha}\n\\end{array}\\]\n\n\n5.5.4 Ejemplo R\n\nVer código R: Wilcoxon y Mann-Whitney.R\nUsar la base de datos Goats.df correspondiente a los datos de las cabras arriba descritos.\nHacer la prueba T de dos muestras y el test de Wilcoxon (R lo llama Wilcoxon pero calcula la estadística de Mann-Whitney).\n\nREVISAR SI EL CÓDIGO R VA EN ESTA PARTE\nFALTAN DIAPOSITIVAS 23-25 CREO QUE SALE DEL CÓDIGO\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "anova.html#test-de-kruskal-wallis",
    "href": "anova.html#test-de-kruskal-wallis",
    "title": "6  Anova una vía",
    "section": "6.1 Test de Kruskal-Wallis",
    "text": "6.1 Test de Kruskal-Wallis\n\n6.1.1 Ejemplo de Hipertensión\nEjemplo: Estudio de hipertensión. 25 pacientes. Tratamientos: Control, dieta baja en sal, dieta sin sal, dosis 1 de un fármaco y dosis 2 fármaco. Respuesta: Las presiones arteriales sistólicas al final del tratamiento.\nPEGAR TABLA DIAPOSITIVA 3\n\n\n6.1.2 Diseños Experimentales\n\nDiseño experimental: se emplea cuando se buscan relaciones entre una variable cuantitativa dependiente (variable respuesta) y una o varias variables cualitativas independientes.\nDiseño completamente aleatorio: los tratamientos o niveles de un factor son aplicados aleatoriamente a un conjunto homogéneo de unidades experimentales.\nVarias poblaciones: Cada tratamiento representa una población de la cual se obtiene una muestra.\nNormalidad y homogeneidad: Se asume que las distribuciones de la respuesta para cada nivel del factor son normales con varianzas iguales.\n\n\n\n6.1.3 Conceptos Fundamentales\n\nFactor:Variable Cualitativa.\nRespuesta:Variable Cuantitativa.\nTratamientos:Niveles del factor.\nUnidad experimental:Individuo sobre el cuál se hace la medición de las variables.\nUnidad muestral:Varias observaciones dentro de la misma unidad experimental.\nReplicas:Número de unidades experimentales por tratamiento.\nError experimental:Unidades experimentales homogéneas tienen respuestas distintas.\nEstructura de tratamientos:Factores involucrados en el análisis (una vía, dos vías, factorial).\nEstructura de diseño: Cómo se asignan las unidades experimentales a los tratamientos (DCA, DBA, Anidado, Parcelas Divididas).\n\n\n\n6.1.4 Evaluación de Supuestos\n\nNormalidad Test de Kolmogorov-Smirnov (Lilliefors) Shapiro-Wilk, Anderson Darling, Cramer von Mises, Jarque-Vera, DAgostino, etc, con las observaciones de cada tratamiento (varias pruebas) o con los residuales del modelo ANOVA.\nHomocedasticidadBartlett, Levene, Cochran, etc, con base en las muestras de los tratamientos.\n\n\n\n6.1.5 Tabla de Datos Diseño a una Vía\nTable: Arreglo de datosen un dise~no experimental con un solo factor de \\(k\\) tratamientos (niveles del factor) y \\(n\\) datos por tratamiento.\nPEGAR TABLA DIAPOSITIVA 8\n\n\n6.1.6 Muestra Aleatoria e Hipótesis\nTable: Arreglo con la muestra aleatoriade un dise~no experimental con un solo factor de \\(k\\) tratamientos (niveles del factor) y \\(n\\) datos por tratamiento.\nPEGAR TABLA DIAPOSITIVA 9\n\\[\\begin{array}{l}H_{0}: \\mu_{1}=\\mu_{2}=\\ldots=\\mu_{k} \\\\ H_{a}: \\mu_{i} \\neq \\mu_{j}\\end{array}\\] ### Hipótesis\nEn términos poblacionales\n\\[\\begin{array}{l}\nH_{0}: Y_{i j}=\\mu+\\epsilon_{i j}, i=1, \\ldots, n ; j=1, \\ldots, k \\\\\nH_{a}: Y_{i j}=\\mu+\\lambda_{j}+\\epsilon_{i j}\n\\end{array}\\]\nEn términos muestrales se tendría\n\\[\\begin{aligned}\nY_{i j} & =\\overline{\\bar{Y}}+e_{i j} \\\\\n& =\\overline{\\bar{Y}}+\\left(Y_{i j}-\\bar{Y}_{j}\\right) \\\\\nY_{i j} & =\\overline{\\bar{Y}}+\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+e_{i j} \\\\\n& =\\overline{\\bar{Y}}+\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right) \\\\\n\\left(Y_{i j}-\\overline{\\bar{Y}}\\right) & =\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right)\n\\end{aligned}\\]\n\n\n6.1.7 Descomposición de la Variabilidad\n\\[\\begin{aligned}\\left(Y_{i j}-\\overline{\\bar{Y}}\\right) & =\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right) \\\\ \\left(Y_{i j}-\\overline{\\bar{Y}}\\right)^{2} & =\\left[\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right)\\right]^{2} \\\\ & =\\left[\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)^{2}+2\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)\\left(Y_{i j}-\\bar{Y}_{j}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right)^{2}\\right] \\\\ \\sum_{i=1}^{n_{j}} \\sum_{j=1}^{k}\\left(Y_{i j}-\\overline{\\bar{Y}}\\right)^{2} & =\\sum_{i=1}^{n_{j}} \\sum_{j=1}^{k}\\left[\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)^{2}+2\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)\\left(Y_{i j}-\\bar{Y}_{j}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right)^{2}\\right] \\\\ & =\\sum_{i=1}^{n_{j}} \\sum_{j=1}^{k}\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)^{2}+\\sum_{i=1}^{n_{j}} \\sum_{j=1}^{k}\\left(Y_{i j}-\\bar{Y}_{j}\\right)^{2} \\\\ S C T & =S C T R+S C E\\end{aligned}\\]\n\n\n6.1.8 Tabla de Análisis de Varianza\nTable: Tabla de descomposición de la varianza para un diseño a una vía, basada en la muestra aleatoria \\(Y_{i j}, i=1, \\ldots, n, j=1, \\ldots, k\\)\nRegión de Rechazo: Se rechaza \\(H_0\\) con un nivel de significancia \\(\\alpha\\) si \\(F&gt;F_{(k-1),(N-k), 1-\\alpha}\\)\n\n\n6.1.9 Tabla de Análisis de Varianza. Datos Observados\nTable: Cálculo de la estadística F. Datos observados: sctr, sce, cmtr, cme, son valores reales (observaciones).\nPEGAR TABLA DIAPOSITIVA 13\nSe rechaza \\(H_0\\) con un nivel de significancia \\(\\alpha\\) si \\(F_{c}&gt;F_{(k-1),(N-k), 1-\\alpha} . F_{c}\\) es el valor calculado de \\(F\\) con base en \\(y_{i j}, i=1, \\ldots, n ; j=1, \\ldots, k\\)\n\n\n6.1.10 Homogeneidad de Varianzas. Prueba de Hartley\nRequiere el mismo \\(n\\)\n\n\\(H_{0}: \\sigma_{1}^{2}=\\ldots=\\sigma_{k}^{2}\\)\n\\(H a: \\sigma_{i}^{2} \\neq \\sigma_{j}^{2}, i, j=1, \\ldots, k, i \\neq j\\)\n\\(F_{\\max }=\\frac{S_{\\max }^{2}}{S_{\\min }^{2}}\\)\nSe rechaza \\(H_0\\) si \\(F_{\\max }&gt;F_{1-\\alpha, k,(n-1)}\\)\n\n\n\n6.1.11 Homogeneidad de Varianzas. Prueba de Bartlett\nPermite diferentes \\(n\\)\n\n\\(H_{0}: \\sigma_{1}^{2}=\\ldots=\\sigma_{k}^{2}\\)\n\\(H a: \\sigma_{i}^{2} \\neq \\sigma_{j}^{2}, i, j=1, \\ldots, k, i \\neq j\\) \\(C=\\left(\\sum_{j=1}^{k}(n-1) \\ln \\bar{S}^{2}-\\sum_{j=1}^{k}(n-1) \\ln S_{j}^{2}\\right), \\mathrm{con}\\) \\(\\\\\\bar{S}^{2}=\\sum_{j=1}^{k}\\) \\(\\frac{S_{j}^{2}}{k}\\)\nSe rechaza \\(H_0\\) si \\(C_{\\max }&gt;\\chi_{\\alpha-1, k}^{2}\\)\n\n\n\n6.1.12 Homogeneidad de Varianzas. Prueba de Levene\n\\(Z_{i j}=\\left|Y_{i j}-\\tilde{Y}_{j}\\right|, \\tilde{Y}_{j}\\) es la mediana de \\(j\\)-ésimo grupo, \\(j=1, \\ldots, k\\)\n\\[\\begin{array}{l}\nL=\\frac{\\sum_{j=1}^{k} n_{j}\\left(\\bar{Z}_{j}-\\bar{Z}\\right)^{2} /(k-1)}{\\sum_{j=1}^{k} \\sum_{i=1}^{n_{i}}\\left(Z_{i j}-\\bar{Z}_{j}\\right)^{2} /\\left(n_{k}-k\\right)} \\\\\nn_{k}=\\sum_{j=1}^{k} n_{j}\n\\end{array}\\]\nSe rechaza \\(H_0\\) si \\(L&gt;F_{(1-\\alpha),(k-1),\\left(n_{k}-1\\right)}\\)\n\n\n6.1.13 Pruebas de Comparación Múltiple\n\nSe utilizan cuando se rechaza la hipótesis nula, para establecer entre cuales de los tratamientos hay diferencias significativas.\nAlgunas de las pruebas comúnmente usadas son LSD (Least Significant Difference), Tukey, Duncan, Schffe y Bonferroni, Dunnet,…\n\n\n\n6.1.14 Diferencia Mínima Significativa (LSD)\n\n\\(S_{R}^{2}:\\): Varianza residual del ANOVA\n\\(\\bar{y}_{j}\\) y \\(n_j\\): Media y el tamaño de muestra del tratamiento \\(j, j=1, \\ldots, k\\)\n\\(t_{\\frac{\\alpha}{2}}\\): Cuantila de una distribución T-student con \\((n-k)\\) g.l\n\\(\\mathrm{LSD}=t_{\\frac{\\alpha}{2}, N-k} \\sqrt{S_{R}^{2}\\left(\\frac{1}{n_{i}}+\\frac{1}{n_{j}}\\right)}\\)\nSi \\(\\left|\\bar{y}_{i}-\\bar{y}_{j}\\right|&gt;\\mathrm{LSD}\\), se rechaza la hipótesis \\(\\mathrm{H}_{0}: \\mu_{i}=\\mu_{j}\\) al nivel de significancia \\(\\alpha\\).\n\n\n\n6.1.15 Prueba de Tukey\n\n\\(H S D=q_{\\alpha}(p, v) \\sqrt{\\frac{C M E}{n}}\\)\n\\(q_{\\alpha}(p, v)\\) es el valor crítico del rango estudentizado de Tukey (valor de tabla) con \\(p\\) igual al número de tratamientos y v los grados de libertad asociados al \\(CME\\).\nOrdene las \\(p\\) medias muestrales. Si la diferencia entre dos medias muestrales es mayor del valor calculado de \\(HSD\\) entonces la conclusión es que hay diferencias entre las medias (poblacionales) de los tratamientos.\nRequiere igual número de replicas por tratamientos (hay un ajuste para tamaños de muestra distintos). https://www.real-statistics.com/statistics-tables/studentized-range-q-table/\n\n\n\n6.1.16 Test de Kruskal-Wallis\nSupuestos: \\(k\\) muestras independientes de distribuciones continuas Hipótesis: La mediana de las \\(k\\) distribuciones es igual\n\n\\(H_{0}: \\theta_{1}=\\theta_{2}=\\ldots=\\theta_{k}\\)\n\\(H_{a}: \\theta_{i} \\neq \\theta_{j}\\), para algún \\(i \\neq j\\)\n\nEstadística de Prueba\nPEGAR “TABLA” DIAPOSITIVA 20\n\\(R_{i j}=\\) Rango de la \\(i\\)-ésima variable del \\(j\\)-ésimo grupo en la muestra combinada \\(Y_{11}, \\ldots, Y_{n_{k} k} . n_{j}\\) es el tamaño de muestra en el grupo \\(j, j=1, \\ldots, k\\)\n\n\n6.1.17 Distribución\n\n6.1.17.1 Revisión de los Supuestos:\n\nLos textos no están de acuerdo respecto al supuesto distribucional. Algunos mencionan que solo se requieren variables ordinales (Conover, Sheskin) y otros que estas deben ser continuas (Hollander and Wolfe, Gibbons, Klake and Mckean)\nKruskal, W. and Wallis,A. 1952. Use of Ranks in One-Criterion Variance Analysis. Use of Ranks in One-Criterion Variance Analysis, JASA, 47:260, 583-621.El test puede hacerse con variables con escala ordinal y usar la distribución exacta. Si se usa la aproximación \\(\\chi_{k-1}^{2}\\) se requiere continuidad.\n\n\n\n\n6.1.18 Hipótesis Sobre \\(k\\) Muestras Independientes\n\\[\\begin{array}{l}\nH=\\frac{12}{N(N+1)} \\sum_{j=1}^{k} n_{j}\\left(\\bar{R}_{j}-\\frac{N+1}{2}\\right)^{2} \\\\\nH=\\frac{12}{N(N+1)}\\left(\\sum_{j=1}^{k} \\frac{R_{j}^{2}}{n_{j}}\\right)-3(N+1), \\quad \\operatorname{con} N=\\sum_{j=1}^{k} n_{j}\n\\end{array}\\]\nRegión de Rechazo:\n\nSi \\(k \\leq 5\\) y \\(n_{j}\\) son pequeños, se usa la distribución exacta (ver tabla A.12 Hollander and Wolfe y 13.13 de Zar).\nSi las muestras son grandes, bajo \\(H_{0}, H \\sim \\chi_{(k-1)}^{2}\\). Se rechaza \\(H_0\\), al nivel \\(\\alpha\\), si \\(H_{c}&gt;\\chi_{1-\\alpha, k-1}^{2}\\)\n\n\n\n6.1.19 Hipótesis Sobre \\(k\\) Muestras Independientes\nEjemplo: Se tiene información tomada de forma aleatoria, sobre la duración en horas de tubos de magnetrón (componentes de los hornos microondas).\n\nlibrary(ggpubr)\n\n\n\ndtubos=c(36, 49, 71, 48, 33, 31, 5, 60, 140, 67,\n          2, 59, 53,  5, 42)\nrank(dtubos)\n\n [1]  6.0  9.0 14.0  8.0  5.0  4.0  2.5 12.0 15.0 13.0  1.0 11.0 10.0  2.5  7.0\n\ngrupo=c(rep(seq(1,3), 5))\ncbind(dtubos, grupo)\n\n      dtubos grupo\n [1,]     36     1\n [2,]     49     2\n [3,]     71     3\n [4,]     48     1\n [5,]     33     2\n [6,]     31     3\n [7,]      5     1\n [8,]     60     2\n [9,]    140     3\n[10,]     67     1\n[11,]      2     2\n[12,]     59     3\n[13,]     53     1\n[14,]      5     2\n[15,]     42     3\n\ndatos2=as.data.frame(cbind(grupo, dtubos))\nggboxplot(datos2, x = \"grupo\", y = \"dtubos\", col=rgb(0,0.5,1), ylab = \"Duración Tubos Magnetrón\", xlab = \"Marca\")\n\n\n\n\n\nggline(datos2, x = \"grupo\", y = \"dtubos\", \n       add = c(\"mean_se\", \"jitter\"), \n       ylab = \"Duraci?n Tubos Magnetrón\", xlab = \"Grupo\")\n\n\n\n\n\nlibrary(dunn.test)\nkruskal.test(dtubos ~ grupo, data = datos2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  dtubos by grupo\nKruskal-Wallis chi-squared = 2.3191, df = 2, p-value = 0.3136\n\n\n\ndunn.test(dtubos, g=grupo, method=\"bonferroni\", kw=TRUE, label=TRUE, alpha=0.05)\n\n  Kruskal-Wallis rank sum test\n\ndata: dtubos and grupo\nKruskal-Wallis chi-squared = 2.3191, df = 2, p-value = 0.31\n\n                         Comparison of dtubos by grupo                         \n                                 (Bonferroni)                                  \nCol Mean-|\nRow Mean |          1          2\n---------+----------------------\n       2 |   0.707738\n         |     0.7187\n         |\n       3 |  -0.813899  -1.521638\n         |     0.6236     0.1921\n\nalpha = 0.05\nReject Ho if p &lt;= alpha/2\n\n\nAGREGAR TABLA DIAPOSITIVA 22\n\\[H=\\frac{12}{(15)(16)}\\left(\\sum_{j=1}^{3} \\frac{R_{j}^{2}}{n_{j}}\\right)-3(16)\\]\n\\[H_{c}=\\frac{12}{(15)(16)}\\left[\\frac{39.5^{2}}{5}+\\frac{29.5^{2}}{5}+\\frac{51^{2}}{5}\\right]-3(16)=2.315\\] Con, \\(\\alpha=5 \\%, \\chi_{2,0.95}^{2}=5.99\\)\n\nNo hay evidencia para rechazar \\(H_0\\). No hay evidencia para rechazar la hipótesis de igualdad de medianas de las 3 distribuciones es la misma.\n\n\n\n6.1.20 Tratamiento de Empates\n\\[H_{\\text {corr }}=\\frac{H}{1-C}, \\quad C=\\frac{\\sum_{i=1}^{m}\\left(t_{i}^{3}-t_{i}\\right)}{N^{3}-N}, H_{\\text {corr }}=2.3191\\] \\(t_i\\): Número de empates en el \\(i\\)-ésimo grupo de empates\n\\(m\\): Número de grupos con rangos empatados\n\n\n6.1.21 Comparaciones Múltiples. Dunn (ver PMCMR)\n\nNo debe usarse test de Wilcoxon de dos muestras porque aumenta la probabilidad de error tipo I.\nCalcule \\(R_{j}, j=1, \\ldots, k\\) Suma de los rangos de cada grupo.\nSe rechaza la hipótesis de que las poblaciones \\(i,j\\) son iguales si\n\n\\[\\left|\\frac{R_{j}}{n_{j}}-\\frac{R_{i}}{n_{i}}\\right|&gt;Z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{N(N+1)}{12}\\left(\\frac{1}{n_{j}}+\\frac{1}{n_{i}}\\right)} i \\neq j, i, j=1,2 \\ldots, k\\]\nSi hay empates\n\\[\\begin{aligned}\n\\left|\\frac{R_{j}}{n_{j}}-\\frac{R_{i}}{n_{i}}\\right| & &gt;Z_{1-\\frac{\\alpha}{2}} \\sqrt{\\left(\\frac{N(N+1)}{12}-B\\right)\\left(\\frac{1}{n_{j}}+\\frac{1}{n_{i}}\\right)} \\\\\nB & =\\frac{\\sum_{i=1}^{m}\\left(t_{i}^{3}-t_{i}\\right)}{12(N-1)}\n\\end{aligned}\\]\n\n\n6.1.22 Ejercicio 1\nSe mide \\(X\\): Número de de moscas por metro cúbico de follaje. Se tienen tres tratamientos (hierba, arbustos y árboles). Pruebe la hipótesis de que la abundancia en las tres capas es la misma, usando el test de Kruskal-Wallis.\nPEGAR TABLA DIAPOSITIVA 27\n\n\n6.1.23 Ejercicio 2\n8 contenedores de agua se toman en cada uno de 4 estanques. Se mide el pH en cada muestra. Pruebe la hipÓtesis de que el pH en todos los estanques es el mismo, usando el test de Kruskal-Wallis.\nPEGAR TABLA DIAPOSITIVA 28\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "friedman.html#tipos-de-diseños-experimentales",
    "href": "friedman.html#tipos-de-diseños-experimentales",
    "title": "7  Diseños en bloques y correlación",
    "section": "7.1 Tipos de Diseños Experimentales",
    "text": "7.1 Tipos de Diseños Experimentales\n\nCompletamente aleatorio: Unidades experimentales homogéneas.\nBloques aleatorios: Unidades experimentales heterogéneas.\nAnidados: Unidades experimentales y muestrales.\nMedidas repetidas: Varias medidas al mismo individuo (generalmente en el tiempo).\nCuadrado Latino: Doble bloqueo.\nBloques incompletos. Más tratamientos que unidades experimentales en algunos de los bloques.\nParcelas divididas: Es un tipo de diseño en bloques incompletos.\n\n\n7.1.1 Diseño en Bloques Aleatorios\n\nUnidades experimentales heterogéneas.\nBloques: Grupo de unidades homogéneas.\nDiseño en bloques: Se forman grupos de unidades homogéneas y después se hace asignacién aleatoria de tratamiento a las unidades dentro de cada bloque.\nModelo: \\(Y_{i j}=\\mu+\\beta_{i}+\\lambda_{j}+\\epsilon_{i j}\\)\n\\(i=1, \\ldots, n\\), bloques, \\(j=1, \\ldots, K\\) tratamientos.\n\n\n7.1.1.1 Ejemplo de Bloques: Consumo de Combustible\nEjemplo: 5 Marcas de vehículos compactos. Y: Millas/Galón. Hay 4 Conductores (bloques). Se quiere probar la hipótesis de igualdad de consumo.\nPEGAR TABLA DIAPOSITIVA 5\n\n\n7.1.1.2 Ejemplo de Bloques: Semillas de Algodón\nEjemplo: 5 Fertilizantes . Y: Rendimiento de semillas de algodón. El terreno se divide en 4 bloques y cada bloque en 5 parcelas. Se fumiga cada parcela con un fertilizante. Se quiere probar la hipótesis de igualdad de fertilizantes.\nPEGAR TABLA DIAPOSITIVA 6\nhttps://alexrojas.netlify.app/post/bloques/\n\n\n7.1.1.3 Ejemplo: Competencia de salto\nEjemplo: 4 jueces califican una competencia de salto que incluye 10 competidores. La calificación toma valores entre 1 y 10.\nPEGAR TABLA DIAPOSITIVA 7\n\n\n7.1.1.4 Ejemplo: Sabor del Vino\nEjemplo: 7 personas califican el sabor de un vino (1: Muy malo. 11: Muy bueno) a tres horas del día. Los participantes no saben que es el mismo vino. Hay diferencias entre las calificaciones según la hora.\nPEGAR TABLA DIAPOSITIVA 8\nhttps: //www.cienciadedatos.net/documentos/21_friedman_test"
  },
  {
    "objectID": "friedman.html#test-de-friedman",
    "href": "friedman.html#test-de-friedman",
    "title": "7  Diseños en bloques y correlación",
    "section": "7.2 Test de Friedman",
    "text": "7.2 Test de Friedman\n\nUn mismo individuo es observado bajo condiciones experimentales distintas.\nEl mismo atleta recibe 4 calificaciones por jueces distintos.\nElmismo participante califica el vino a tres horas distintas.\nNota: No debe confundirse con diseños de medidas repetidas (tiempo) y anidados (unidades experimentales y unidades muestrales), donde para el mismo individuo hay varias mediciones bajo las mismas condiciones experimentales.\nFriedman, M. (1937). The use of ranks to avoid the assumption of normality implicit in the analysis of variance.JASA, 32, 675{701.\nGeneralizael test del Signo de muestras pareadas a más de dos tratamientos.\nHay independencia entre individuos, pero no entre tratamientos\n\\(K\\) tratamientos, \\(B\\) bloques\nHipótesis: \\(H_0\\): Igualdad de medianas. Ha: Diferencia entre al menos un par de medianas.\nVariable: Ordinal (ver Artículo de Friedman)\n\n\n7.2.1 Tabla de datos\nTable: Arreglo de datos suponiendo que hay una única observación (individuo) por bloque\nPEGAR TABLA DIAPOSITIVA 11\n\n\n7.2.2 Hipótesis\nTable: Arreglo la muestra aleatoria de un diseño experimental con un solo factor de \\(k\\) tratamientos (niveles del factor) y \\(n\\) datos por tratamiento.\nPEGAR TABLA DIAPOSITIVA 12\n\\[\\begin{array}{l}H_{0}: \\theta_{1}=\\theta_{2}=\\ldots=\\theta_{k} \\\\ H_{a}: \\theta_{i} \\neq \\theta_{j}\\end{array}\\]\n\n\n7.2.3 Hipótesis y estadística\nEn términos poblacionales: Suponga que \\(\\theta\\) es la mediana total\n\\[\\begin{array}{l}\nH_{0}: Y_{i j}=\\theta+\\epsilon_{i j}, i=1, \\ldots, n ; j=1, \\ldots, k \\\\\nH_{a}: Y_{i j}=\\theta+\\beta_{i}+\\lambda_{j}+\\epsilon_{i j},\n\\end{array}\\]\ndonde las variables \\(\\epsilon_{i j}\\) son una m.a.s. de una distribución continua con mediana 0. Sea \\(R_{i j}\\) denota el rango de \\(Y_{i j}\\) en el \\(i\\)-ésimo bloque \\(Y_{i 1}, \\ldots, Y_{i k}\\)\n\\[\\begin{aligned}\nR_{j} & =\\sum_{i=1}^{n} R_{i j}, \\quad \\bar{R}_{j}=\\frac{R_{j}}{n} \\\\\nS & =\\frac{12}{n k(k+1)}\\left(\\sum_{j=1}^{k} R_{j}^{2}\\right)-3 n(k+1)\n\\end{aligned}\\]\n\n\n7.2.4 Tratamiento de Empates\n\\[S_{\\mathrm{emp}}=\\frac{S}{C}, \\quad C=1-\\frac{\\sum_{i=1}^{m}\\left(t_{i}^{3}-t_{i}\\right)}{n\\left(k^{3}-k\\right)}\\]\n\n\\(t_i\\): Número de empates en el \\(i\\)-ésimo grupo de empates\n\\(m\\): Número de grupos con rangos empatados\n\n\n\n7.2.5 Decisión\n\nDistribución exacta ver Hollander, Wolfe and Chicken, 2014.\nDistribución asintótica\n\n\\[S \\sim \\chi_{k-1}^{2}\\]\nSe rechaza \\(H_0\\) si \\(S&gt;\\chi_{k-1,1-\\alpha}^{2}\\)\n\nEmpates: Se toma el rango promedio."
  },
  {
    "objectID": "friedman.html#comparación-múltiple.-nemenyi",
    "href": "friedman.html#comparación-múltiple.-nemenyi",
    "title": "7  Diseños en bloques y correlación",
    "section": "7.3 Comparación múltiple. Nemenyi",
    "text": "7.3 Comparación múltiple. Nemenyi\n\\[\\left|\\bar{R}_{i}-\\bar{R}_{j}\\right|&gt;\\frac{q_{\\infty, k, \\alpha}}{\\sqrt{2}} \\sqrt{\\frac{k(k+1)}{6 n}}\\] - \\(q_{\\infty, k, \\alpha}\\) es la cuantila de la distribución del rango estudentizado (ver “PMCMR.pdf”)\n\nTabla del rango estudentizado http://www.real-statistics.com/statistics-tables/studentized-range-q-table/\n\n\n7.3.1 Ejemplo: Ejemplo de Jueces\nEjemplo: Rangos de las calificaciones dadas por los jueces\nPEGAR TABLA DIAPOSITIVA 17\n\\[\\begin{aligned} R_{1} & =25, R_{2}=34.5, R_{3}=14, R_{4}=26.5 \\\\ S_{C} & =\\frac{12\\left(25^{2}+34.5^{2}+14^{2}+26.5^{2}\\right)}{(10)(4)(5)}-3(10)(5)=12.81 \\\\ C & =1-\\frac{\\left(2^{3}-2\\right)+\\left(2^{3}-2\\right)+\\left(2^{3}-2\\right)}{10\\left(4^{3}-4\\right)}=0.97 \\\\ S_{\\mathrm{emp}} & =\\frac{S}{C}=13.2 \\\\ \\chi_{0.01,3}^{2} & =11.34\\end{aligned}\\]\n\nConclusión: Hay diferencias significativas entre las calificaciones de los jueces\nEntre cuáles jueces hay diferencias?. Use Nemenyi"
  },
  {
    "objectID": "friedman.html#test-de-mack-skillings-bloques-con-varias-unidades",
    "href": "friedman.html#test-de-mack-skillings-bloques-con-varias-unidades",
    "title": "7  Diseños en bloques y correlación",
    "section": "7.4 Test de Mack-Skillings: Bloques con varias unidades",
    "text": "7.4 Test de Mack-Skillings: Bloques con varias unidades\nTable: Arreglo de datos suponiendo que hay \\(c\\) observaciones por bloque\nPEGAR TABLA DIAPOSITIVA 19\nMack G. & Skillings J (1980). A Friedman-type rank test for main effects in a two-factor ANOVA. Journal of the American Statistical Association, 75(372), 947-951\n\n7.4.1 Ejemplo: Mack-Skillings\nTratamientos: Laboratorios.Bloques: Muestras de cereal enriquecido con 0, 4 y 8 mg/100gr de Niacina.Respuesta: mg/100gr de Niacina\nPEGAR TABLA DIAPOSITIVA 20\n\n\n7.4.2 Ejemplo de Mack-Skillings. Tabla Transpuesta\nTable: Respuesta: mg/100gr de Niacina. 4 tratamientos (laboratorios), 3 bloques (enriquecimiento de Niacina) y 3 réplicas.\nPEGAR TABLA DIAPOSITIVA 21\n\n\n7.4.3 Estadística: Mack-Skillings\nIMAGEN DE EJERCICIOS DIAPOSITIVA 22\nUsando la ecuación 7.57 se tiene: \\(k = 4\\) tratamientos. \\(n = 3\\) bloques. \\(c = 3\\) número de réplicas dentro de bloque.\n\\[\\begin{aligned} S_{1} & =17.67, S_{2}=30.05, S_{3}=15.83, S_{4}=14 \\\\ N & =36 ; c=3 ; k=4, n=3 . \\\\ M S & =\\left(\\frac{12}{4(36+3)}\\right)\\left[(17.67)^{2}+(30.5)^{2}+(15.83)^{2}+(14)^{2}\\right] \\\\ & -3(36+3)=12.93 . \\\\ \\text { ValorP } & =1-\\text { pchisq }(12.93,3) . \\\\ & =0.0047 .\\end{aligned}\\]"
  },
  {
    "objectID": "friedman.html#correlación-de-pearson",
    "href": "friedman.html#correlación-de-pearson",
    "title": "7  Diseños en bloques y correlación",
    "section": "7.5 Correlación de Pearson",
    "text": "7.5 Correlación de Pearson\n\\[\\mathrm{R}_{X Y}=\\frac{\\operatorname{Cov}(X, Y)}{S_{X} S_{y}}=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)} \\sqrt{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)}}\\] Inferencia\n\\[\\mathrm{T}=\\frac{\\mathrm{R}_{X Y}-\\rho_{X Y n h}}{\\sqrt{\\frac{1-R_{X Y}^{2}}{n-2}}} \\sim \\text { t-student }_{n-2}\\]\n\n7.5.1 Relación entre \\(T\\) y \\(R_XY\\)\nPEGAR IMAGEN DIAPOSITIVA 25"
  },
  {
    "objectID": "friedman.html#pearson-y-spearman",
    "href": "friedman.html#pearson-y-spearman",
    "title": "7  Diseños en bloques y correlación",
    "section": "7.6 Pearson y Spearman",
    "text": "7.6 Pearson y Spearman\nPEGAR IMAGEN DIAPOSITIVA 26"
  },
  {
    "objectID": "friedman.html#correlación-de-spearman",
    "href": "friedman.html#correlación-de-spearman",
    "title": "7  Diseños en bloques y correlación",
    "section": "7.7 Correlación de Spearman",
    "text": "7.7 Correlación de Spearman\n\nNo requiere normalidad bivariada. Puede usarse con variables discretas\nDetecta relación monótona (creciente o decreciente)\nEs resistente a observaciones atípicas porque se define en términos de rangos\n\nPEGAR TABLA DIAPOSITIVA 27\nEstadística\n\\[\\mathrm{R}_{S}=\\frac{\\operatorname{Cov}(\\mathrm{R}(X), \\mathrm{R}(Y))}{S_{R(X)} S_{R(Y)}}=\\frac{\\sum_{i=1}^{n}\\left(\\mathrm{R}\\left(X_{i}\\right)-\\frac{n+1}{2}\\right)\\left(\\mathrm{R}\\left(Y_{i}\\right)-\\frac{n+1}{2}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(\\mathrm{R}\\left(X_{i}\\right)-\\frac{n+1}{2}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(\\mathrm{R}\\left(Y_{i}\\right)-\\frac{n+1}{2}\\right)^{2}}}\\] Nota:\n\\[\\begin{array}{l}\n\\bar{R}(X)=\\frac{1}{n} \\sum_{i=1}^{n} R\\left(X_{i}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} i=\\frac{n(n+1)}{2 n}=\\frac{(n+1)}{2} \\\\\n\\bar{R}(Y)=\\frac{1}{n} \\sum_{i=1}^{n} R\\left(Y_{i}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} i=\\frac{n(n+1)}{2 n}=\\frac{(n+1)}{2}\n\\end{array}\\]\nLa estadística se puede simplificar como (si no hay empates)\n\\[\\mathrm{R}_{S}=1-\\frac{6 \\sum_{i=1}^{n} D_{i}^{2}}{n^{3}-n}\\]\nRegión de rechazo Valores críticos de la tabla de coeficientes de correlación de Spearman (Conover A.10, Hollander A.31 y Zar 13.20)\nhttp://www.real-statistics.com/statistics-tables/spearmans-rho-table/\nPEGAR TABLA DIAPOSITIVA 30\n\\[\\begin{array}{l}r_{s}=1-\\frac{6 \\sum_{i=1}^{n} d_{i}^{2}}{5^{3}-5} \\\\ r_{s}=1-\\frac{6 \\times 0}{(120)}=1 ; \\quad r_{s}=1-\\frac{6 \\times 40}{(120)}=-1\\end{array}\\] ### Aproximación asintótica\n\\[\\begin{array}{l}\n\\mathbb{E}\\left(R_{s}\\right)=0, \\mathbb{V}\\left(R_{s}\\right)=\\frac{1}{(n-1)} \\text {. Comprobarlo } \\\\\nZ=\\frac{R_{s}}{\\sqrt{\\frac{1}{(n-1)}}} \\sim N(0,1) \\\\\n\\end{array}\\]\nRegión de rechazo Se rechaza \\(H_0\\) al nivel \\(\\alpha\\) si \\(z_{c}&gt;z_{1-\\frac{\\alpha}{2}} \\circ z_{c}&lt;z_{\\frac{\\alpha}{2}}\\)\n\\[T=\\frac{R_{s}}{\\sqrt{\\frac{1-R_{s}^{2}}{(n-2)}}} \\sim T_{n-2}\\]\nRegión de rechazo Se rechaza \\(H_0\\) al nivel \\(\\alpha\\) si \\(t_{c}&gt;t_{1-\\frac{\\alpha}{2}} \\circ t_{s} c&lt;t_{\\frac{\\alpha}{2}}\\)\n\n7.7.1 Ejemplo\n\nDos jueces clasifican 8 vinos como peor (1) a mejor (8).\n\nPEGAR TABLA DIAPO 32\n\nExiste relación entre lo dicho por los dos jueces?\n\n\\[\\begin{array}{c}\nr_{s}=1-\\frac{6[4+9+1+4+1+1+4]}{8(63)} \\\\\nr_{s}=1-\\frac{144}{504}=0.7143\n\\end{array}\\]\n\n\\(n=8 \\alpha=5 \\%\\) Valor critico 0.738. No hay evidencia para rechazar \\(H_0\\).\n\n\n\n7.7.2 Ejercicio\nSe mide \\(X\\): Colágeno (mg/g peso seco) y \\(Y\\) prolina libre (INSERTAR MICRO moles/g peso seco) en 7 pacientes con cirrosis\nPEGAR TABLA DIAPOSITIVA 33\n\nHay correlación entre \\(X\\) y \\(Y\\)?. Use el coeficiente de Spearman\nCómo se define \\(R_s\\) cuando hay empates"
  },
  {
    "objectID": "friedman.html#correlación-de-kendall",
    "href": "friedman.html#correlación-de-kendall",
    "title": "7  Diseños en bloques y correlación",
    "section": "7.8 Correlación de Kendall",
    "text": "7.8 Correlación de Kendall\nTable: Muestra Aleatoria y Muestra Aleatoria Observada Dos variables cuantitativas continuas\nPEGAR TABLA DIAPOSITIVA 34\n\n7.8.1 Concordancia y Discordancia\n\n\\(\\left(X_{i}, Y_{i}\\right)\\) Y \\(\\left(X_{j}, Y_{j}\\right), i, j=1, \\ldots, n, i \\neq j\\), se llaman concordantes si:\n\n\\[\\begin{array}{l}\nX_{i}&gt;X_{j} \\text { y } Y_{i}&gt;Y_{j} \\circ \\\\\nX_{i}&lt;X_{j} \\text { y } Y_{i}&lt;Y_{j}\n\\end{array}\\]\n\n\\(\\left(X_{i}, Y_{i}\\right)\\) Y \\(\\left(X_{j}, Y_{j}\\right), i, j=1, \\ldots, n, i \\neq j\\), se llaman discordantes si se dan las siguientes relaciones:\n\n\\[\\begin{array}{l}\nX_{i}&gt;X_{j \\text { y }} Y_{i}&lt;Y_{j} \\circ \\\\\nX_{i}&lt;X_{j} \\text { y } Y_{i}&gt;Y_{j}\n\\end{array}\\]\n\n\n7.8.2 Estadística y Distribución\n\nSea \\(N_c\\)= Número de parejas concordantes.\nSea \\(N_d\\)= Número de parejas discordantes.\n\n\\(\\tau=\\frac{N_{c}-N_{d}}{N_{c}+N_{d}}=\\frac{N_{c}-N_{d}}{n(n-1) / 2} \\text {, si no hay empates }\\) si no hay empates.\n\nSi no hay tendencia a la concordancia ni a la discordancia \\(\\tau \\rightarrow 0\\)\nSi hay tendencia a la concordancia \\(\\tau \\rightarrow 1\\)\nSi hay tendencia a la discordancia \\(\\tau \\rightarrow 1\\)\n\n\n\n7.8.3 Kendall. Empates\nTambién llamado coeficiente gamma. Deja solo empates en la respuesta.\n\nSea \\(N_c\\)= Número de parejas concordantes.\nSea \\(N_d\\)= Número de parejas discordantes.\nSi \\(\\left(Y_{i}=Y_{j}\\right)\\) y \\(X_{i} \\neq X_{j}\\) suma \\(\\frac{1}{2}\\) a \\(N_c\\) y \\(N_d\\).\nSi \\(\\left(X_{i}=X_{j}\\right)\\) la pareja \\((i, j)\\) no es tenida en cuenta y el coeficiente se define como\n\n\\(\\tau=\\frac{N_{c}-N_{d}}{N_{c}+N_{d}}, \\quad \\operatorname{con}\\left(N_{c}+N_{d}\\right) \\neq \\frac{n(n-1)}{2}\\)\n\n\n7.8.4 Kendall. Hipótesis\n\\(H_{0}: \\tau=0\\) Las variables son independientes \\(H_{a}: \\tau&gt;0\\) Hay correlación positiva \\(\\tau&lt;0\\) Hay correlación inversa \\(\\tau \\neq 0\\) Hay correlación\n\nValores críticos: Tabla A.11 de Conover\nhttp://www.real-statistics.com/statistics-tables/kendalls-tau-table/\n\n\n\n7.8.5 Kendall. Distribución Asintótica\n\nLa aproximación normal es (Maestría. Comprobarlo)\n\n\\[Z=\\frac{\\tau-E(\\tau)}{\\frac{\\sqrt{2(n+5)}}{3 \\sqrt{n(n-1)}}}\\]\n\nSe rechaza a nivel \\(\\alpha\\)\n\n\\[\\left|\\tau_{c}\\right|&gt;z_{1-\\alpha / 2} \\times \\frac{\\sqrt{2(n+5)}}{3 \\sqrt{n(n-1)}}\\]\n\n\n7.8.6 Ejercicio\nSe mide \\(X\\): Colágeno (mg/g peso seco) y Y prolina libre (MICRO moles/g peso seco) en 7 pacientes con cirrosis.\nAGREGAR TABLA DIAPOSITIVA 40\nHay correlación entre \\(X\\) y \\(Y\\)?. Use el coeficiente de Kendall.\n\n\n7.8.7 Tarea\n\nEstudiar, a través de simulación, la potencia de las tres pruebas (Pearson, Spearman, Kendall) bajo normalidad bivariada.\nRealizar el mismo ejercicio anterior asumiendo otra distribución conjunta.\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "bondad.html#gráfico-cuantil-cuantil",
    "href": "bondad.html#gráfico-cuantil-cuantil",
    "title": "8  Pruebas de Bondad de Ajuste",
    "section": "8.1 Gráfico Cuantil-Cuantil",
    "text": "8.1 Gráfico Cuantil-Cuantil\nPermite observar cuan cerca está la distribución de un conjunto de datos a alguna distribución ideal ó comparar la distribución de dos conjuntos de datos.\n\nSe ordena la muestra \\(x_{1}, \\ldots, x_{n}\\)\nFunción de distribución empírica \\(\\hat{F}\\left(x_{i}\\right)=\\frac{i}{(n+1)}\\), con \\(i\\) la posición que ocupa \\(X_i\\) en la muestra ordenada\nSe compara \\(\\hat{F}\\left(x_{i}\\right)\\) con la función de distribución bajo un modelo de referencia \\(F(x)\\)"
  },
  {
    "objectID": "bondad.html#gráficos-cuantil-cuantil-y-de-probabilidad",
    "href": "bondad.html#gráficos-cuantil-cuantil-y-de-probabilidad",
    "title": "8  Pruebas de Bondad de Ajuste",
    "section": "8.2 Gráficos Cuantil-Cuantil y de Probabilidad",
    "text": "8.2 Gráficos Cuantil-Cuantil y de Probabilidad\n\\(X_{1}, \\ldots, X_{n}\\) muestra aleatoria \\(X_{(1)}, \\ldots, X_{(n)}\\) Estadísticas de orden \\(\\mathrm{QQ}\\) versus \\(F^{-1}\\left(\\frac{i}{n+1}\\right)\\) Gráfico de Probabilidad. \\(F\\left(X_{(i)}\\right)\\) versus \\(\\left(\\frac{i}{n+1}\\right)\\)"
  },
  {
    "objectID": "bondad.html#qq-plot",
    "href": "bondad.html#qq-plot",
    "title": "8  Pruebas de Bondad de Ajuste",
    "section": "8.3 QQ Plot",
    "text": "8.3 QQ Plot\nPEGAR GRÁFICO DIAPOSITIVA 6"
  },
  {
    "objectID": "bondad.html#prueba-de-shapiro-wilk",
    "href": "bondad.html#prueba-de-shapiro-wilk",
    "title": "8  Pruebas de Bondad de Ajuste",
    "section": "8.4 Prueba de Shapiro-Wilk",
    "text": "8.4 Prueba de Shapiro-Wilk\n\nHipótesis: \\(H_0\\): Normalidad. \\(H_1\\): No normalidad. Muestra: \\(X_{1}, \\ldots, X_{n}\\)\n\\(a_i\\) Coeficientes tabla de Shapiro-Wilk (ver enlace). Valores críticos de W (ver enlace).\nNumerador y denominador son proporcionales a estimadores consistentes de \\(\\sigma^{2}\\) bajo normalidad (Bagdonavicius, V., Kruopis, J. and Nikulin, M. 2011. Non-parametric tests for complete data. 2011. Wiley. (páginas 250-253))\n\n\\[W=\\frac{\\left(\\sum_{j=1}^{k} a_{j}\\left(X_{n-j+1}-X_{j}\\right)\\right)^{2}}{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}}, k=\\left\\{\\begin{array}{ll}\n\\frac{n}{2} & \\text { n par } \\\\\n\\frac{n-1}{2} & \\text { n impar }\n\\end{array}\\right.\\]\nSe rechaza \\(H_0\\) si \\(W_c\\) es pequeño, es decir \\(P\\left(W&lt;W_{c}\\right)&lt;\\alpha\\)\nhttp://www.real-statistics.com/statistics-tables/shapiro-wilk-table/"
  },
  {
    "objectID": "bondad.html#ejemplo-shapiro-wilk",
    "href": "bondad.html#ejemplo-shapiro-wilk",
    "title": "8  Pruebas de Bondad de Ajuste",
    "section": "8.5 Ejemplo Shapiro-Wilk",
    "text": "8.5 Ejemplo Shapiro-Wilk\nPEGAR TABLA DIAPOSITIVA 8\nhttps://www.real-statistics.com/tests-normality-and-symmetry/statistical-tests-normality-symmetry/shapiro-wilk-test/\nPEGAR TABLA DIAPOSITIVA 9\nhttps://www.real-statistics.com/tests-normality-and-symmetry/statistical-tests-normality-symmetry/shapiro-wilk-test/"
  },
  {
    "objectID": "bondad.html#prueba-de-kolmogorov-smirnov-distribución-especificada",
    "href": "bondad.html#prueba-de-kolmogorov-smirnov-distribución-especificada",
    "title": "8  Pruebas de Bondad de Ajuste",
    "section": "8.6 Prueba de Kolmogorov-Smirnov Distribución Especificada",
    "text": "8.6 Prueba de Kolmogorov-Smirnov Distribución Especificada\n\nHipótesis: \\(H_{0}: F(x)=F_{0}(x) . H_{1}: F(x) \\neq F_{0}(x)\\)\nMuestra: \\(X_{1}, \\ldots, X_{n}\\)\nFunción de distribución empírica: \\(X_{1}, \\ldots, X_{n}\\) m.a \\(X_{(1)}, \\ldots, X_{(n)}\\) estadísticas de orden.\n\n\\[S_{n}(x)=\\left\\{\\begin{array}{ll}\n0 & x&lt;X_{(1)} \\\\\n\\frac{k}{n} & X_{(k)} \\leq x&lt;X_{(k+1)} \\\\\n1 & x \\geq X_{(n)}\n\\end{array}\\right.\\]\nEstadística:\n\\(D=\\operatorname{máx}_{i=1, \\ldots, n}\\left|S_{n}(x)-F_{0}(x)\\right|=\\text { máx }_{i=1, \\ldots, n}\\left|\\frac{i}{n}-F_{0}\\left(x_{(i)}\\right)\\right|\\)\nDecisión: Se rechaza \\(D \\geq d_{\\alpha}\\)\n\nValores críticos: Ver tabla A.38 de Hollander y Wolfe\nVer también: http://www.real-statistics.com/statistics-tables/kolmogorov-smirnov-table/\nPara n grande D sigue una distribución Kolmogorov http://www.real-statistics.com/tests-normality-and-symmetry/ statistical-tests-normality-symmetry/kolmogorov-smirnov-test/kolmogorov-distribution/\n\n\n8.6.1 Ejemplo\nPEGAR TABLA DIAPOSITIVA 12\nProbar que \\(Z \\sim N(14,2)\\)\n\n\n8.6.2 Tabla de datos. Distribución Especificada\nPEGAR TABLA DIAPOSITIVA 13\nValor crítico: http://www.real-statistics.com/statistics-tables/kolmogorov-smirnov-table/\n\nSe usan las estimaciones de los parámetros de la distribución de referencia. Para el ejemplo deben estimarse \\(\\mu\\) y \\(\\sigma\\)\n\n\\[D=\\text { máx }_{i=1, \\ldots, n}\\left|S_{n}(x)-\\hat{F}_{0}(x)\\right|=\\left|\\frac{i}{n}-\\hat{F}_{0}\\left(x_{i}\\right)\\right|\\]\n\nDecisión: Se rechaza \\(H_0\\) si \\(D \\geq d_{\\alpha}\\)\nValores críticos: Ver tabla A.38 de Hollander y Wolfe.\nVer también: http://www.real-statistics.com/statistics-tables/kolmogorov-smirnov-table/\n\n\n\n8.6.3 Tabla de datos. Distribución No Especificada\nPEGAR TABLA DIAPOSITIVA 15\nValor crítico: http://www.real-statistics.com/statistics-tables/kolmogorov-smirnov-table/"
  },
  {
    "objectID": "bondad.html#prueba-de-normalidad-de-lilliefors",
    "href": "bondad.html#prueba-de-normalidad-de-lilliefors",
    "title": "8  Pruebas de Bondad de Ajuste",
    "section": "8.7 Prueba de Normalidad de Lilliefors",
    "text": "8.7 Prueba de Normalidad de Lilliefors\n\nSe basa en una modificación de la estadística de Kolmogorov-Smirnov arriba descrita\n\n\\[\\begin{aligned} D & =\\operatorname{máx}\\left(D^{+}, D^{-}\\right) \\\\ D^{+} & =\\operatorname{máx}\\left|\\left(\\frac{i}{n}-F_{0}\\left(X_{(i)}\\right)\\right)\\right| \\\\ D^{-} & =\\operatorname{máx}\\left|\\left(\\frac{i-1}{n}-F_{0}\\left(X_{(i)}\\right)\\right)\\right|\\end{aligned}\\]\nSi los parámetros son estimados\n\\[\\begin{aligned}\nD & =\\operatorname{máx}\\left(D^{+}, D^{-}\\right) \\\\\nD^{+} & =\\operatorname{máx}\\left|\\left(\\frac{i}{n}-\\hat{F}_{0}\\left(X_{(i)}\\right)\\right)\\right| \\\\\nD^{-} & =\\operatorname{máx}\\left|\\left(\\frac{i-1}{n}-\\hat{F}_{0}\\left(X_{(i)}\\right)\\right)\\right|\n\\end{aligned}\\]\nTarea: Realizar el test de Lilliefors con los datos del ejemplo Valor crítico: http://WWW.real-statistics.com/statistics-tables/illiefors-test-table/"
  },
  {
    "objectID": "bondad.html#prueba-de-bondad-de-ajuste-chi2",
    "href": "bondad.html#prueba-de-bondad-de-ajuste-chi2",
    "title": "8  Pruebas de Bondad de Ajuste",
    "section": "8.8 Prueba de Bondad de Ajuste \\(\\chi^{2}\\)",
    "text": "8.8 Prueba de Bondad de Ajuste \\(\\chi^{2}\\)\n\nCompara frecuencias observadas con esperadas bajo un modelo. Se usa en bondad de ajuste y en tablas de contingencia. En esta sección se muestra como emplearla en Bondad de Ajuste.\nHipótesis: \\(H_{0}: F(x)=F_{0}(x) . H_{1}: F(x) \\neq F_{0}(x)\\)\n\nEstadística\n\\[X^{2}=\\sum_{i=1}^{k}\\left(\\frac{O_{i}-E_{i}}{\\sqrt{E_{i}}}\\right)^{2} \\sim \\chi_{(k-p-1)}^{2}\\]\n\nSe rechaza la hipótesis nula si \\(X_{c}^{2}&gt;\\chi_{(k-p-1),(1-\\alpha)}^{2}, \\mathrm{p}\\) Número de parámetros estimados.\n\n\n8.8.1 Ejemplo. Variable Poisson\nSe quiere probar que \\(X \\sim \\operatorname{Poisson}(\\hat{\\lambda}), \\hat{\\lambda}=\\bar{x}=2.428\\) \\(E=N \\times P_{j}\\) O: Frecuencia Observada. E: Frecuencia Esperada. \\(X_{c}^{2}=6.087&gt;\\chi_{0.95,5}^{2}=11.07\\), valor \\(-\\mathrm{p}=0.2978\\)\nPEGAR TABLA DIAPOSITIVA 19\n\n8.8.1.1 Datos de una Poisson\nGRÁFICA DIAPOSITIVA 20\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Tcontigencia.html#test-exacto-de-fisher",
    "href": "Tcontigencia.html#test-exacto-de-fisher",
    "title": "9  Tablas de Contingencia",
    "section": "9.1 Test Exacto de Fisher",
    "text": "9.1 Test Exacto de Fisher\n\nExacto: Las probabilidades se calculan encontrando todos los posibles resultados de la estadística de prueba (todas las posibles tablas) y no a partir de un modelo de probabilidad.\nAsociación entre dos variables categóricas. Generalmente se usa para tablas 2 X 2. Puede ser usada también en tablas 2 X \\(k\\).\nSe usa cuando los totales de filas y columnas son fijos.\n\nTable: Tabla 2 X 2. Aplica sobre una tabla de contingencia con totales de fillas y columnas fijos.\nPEGAR TABLA DIAPO 10\n\n9.1.1 Tablas de Contingencia\nTable: Tabla 2 X 2con los 2 marginales fijos. Dados los marginales fijos, cuáles serán los posibles valores de a?\nPEGAR TABLA DIAPO 11\nEncuentre todos los posible valores de a que permitan completar la tabla. Solución: \\(a = 3; 4; 5; 6 : :: ; 14\\). Hay 12 posibles resultados de \\(a\\).\n\\(X\\)= Número de observaciones en la fila 1 y la columna 1. El valor observado de a es 12. Cuál es la probabilidad de este valor si los marginales son fijos?. Cuál es la probabilidad de un valor mayor a 12?\n\\[\\begin{aligned} P(X=a) & =\\frac{\\left(\\begin{array}{c}a+b \\\\ a\\end{array}\\right)\\left(\\begin{array}{c}c+d \\\\ c\\end{array}\\right)}{\\left(\\begin{array}{c}N \\\\ a+c\\end{array}\\right)} \\\\ P(X=12) & =\\frac{\\left(\\begin{array}{c}19 \\\\ 12\\end{array}\\right)\\left(\\begin{array}{c}11 \\\\ 2\\end{array}\\right)}{\\left(\\begin{array}{c}30 \\\\ 14\\end{array}\\right)}=0.019057 \\\\ P(X \\geq 12) & =\\sum_{j=0}^{2} \\frac{\\left(\\begin{array}{c}19 \\\\ 14-j\\end{array}\\right)\\left(\\begin{array}{c}11 \\\\ j\\end{array}\\right)}{\\left(\\begin{array}{c}30 \\\\ 14\\end{array}\\right)}=0.0211\\end{aligned}\\] - Se rechaza la hipótesis nula si el valor de a observado es extremo (muy grande o muy pequeño), es decir si\n\\(P(X \\geq a) \\leq \\alpha\\) (hipótesis de una cola superior) o si \\(P(X \\leq a) \\leq \\alpha\\) (hipótesis de una cola inferior).\n\nValor \\(P\\) dos colas. No hay una expresión similar a cuando se usa una distribución continua (por ejemplo la normal). \\(R\\) usa la aproximación.\n\n\\[\\begin{aligned}\n\\text { Valor } P & =\\sum_{i} P\\left(X=x_{i}\\right) \\\\\n\\operatorname{con} P\\left(X=x_{i}\\right) & \\leq P(X=a)\n\\end{aligned}\\]\n\n\n9.1.2 Tablas de Contingencia. Cambiando el orden de las columnas\nTable: Tabla 2 X 2con los 2 marginales fijos.Se cambia el orden respecto a la corriente del río.\nPEGAR TABLA DIAPO 14\nEncuentre todos los posible valores de a que permitan completar la tabla. Solución: \\(a = 5; 6; 7; 8 : :: ; 16\\). Hay 12 posibles resultados de \\(a\\).\n\\(X=\\) Número de observaciones en la fila 1 y la columna 1. El valor observado de a es 7. Cuál es la probabilidad de este valor si los marginales son fijos?. Cuál es la probabilidad de un valor menor \\(a\\) 7?\n\\[\\begin{array}{l}\nP(X=a)=\\frac{\\left(\\begin{array}{c}\na+b \\\\\na\n\\end{array}\\right)\\left(\\begin{array}{c}\nc+d \\\\\nc\n\\end{array}\\right)}{\\left(\\begin{array}{c}\nN \\\\\na+c\n\\end{array}\\right)} \\\\\nP(X=7)=\\frac{\\left(\\begin{array}{c}\n19 \\\\\n7\n\\end{array}\\right)\\left(\\begin{array}{c}\n11 \\\\\n9\n\\end{array}\\right)}{\\left(\\begin{array}{c}\n30 \\\\\n16\n\\end{array}\\right)}=0.019057 \\\\\nP(X \\leq 7)=\\sum_{j=0}^{7} \\frac{\\left(\\begin{array}{c}\n19 \\\\\n16-j\n\\end{array}\\right)\\left(\\begin{array}{c}\n11 \\\\\nj\n\\end{array}\\right)}{\\left(\\begin{array}{c}\n30 \\\\\n16\n\\end{array}\\right)}=0.0211\n\\end{array}\\]\nConclusión: No depende del orden. El mismo resultado se obtiene\n\n\n9.1.3 Ahora la Especie en las Columnas\nTable: Tabla 2 X 2con los 2 marginales fijos. Se pone la especie en columna.\nPEGAR TABLA DIAPOSI 16\nEncuentre todos los posible valores de a que permitan completar la tabla. Solución: \\(a = 3; 4; 5; 6 : :: ; 14\\). Hay 12 posibles resultados de \\(a\\).\n\\(X=\\) Número de observaciones en la fila 1 y la columna 1. El valor observado de a es 12. Cuál es la probabilidad de este valor si los marginales son fijos?. Cuál es la probabilidad de un valor menor \\(a 12\\)?\n\\[\\begin{aligned} P(X=a) & =\\frac{\\left(\\begin{array}{c}a+b \\\\ a\\end{array}\\right)\\left(\\begin{array}{c}c+d \\\\ c\\end{array}\\right)}{\\left(\\begin{array}{c}N \\\\ a+c\\end{array}\\right)} \\\\ P(X=12) & =\\frac{\\left(\\begin{array}{c}14 \\\\ 12\\end{array}\\right)\\left(\\begin{array}{c}16 \\\\ 7\\end{array}\\right)}{\\left(\\begin{array}{c}30 \\\\ 19\\end{array}\\right)}=0.019057 \\\\ P(X \\geq 12) & =\\sum_{j=0}^{2} \\frac{\\left(\\begin{array}{c}14 \\\\ 16-j\\end{array}\\right)\\left(\\begin{array}{c}16 \\\\ j\\end{array}\\right)}{\\left(\\begin{array}{c}30 \\\\ 19\\end{array}\\right)}=0.0211\\end{aligned}\\]\nConclusión: Da el mismo resultado independientemente de cuál variable está en la columna y cuál en la fila.\n\n\n9.1.4 Test Exacto de Fisher. Aproximación Normal\n\\[Z=\\frac{a-\\frac{(a+b)(a+c)}{N}}{\\sqrt{\\frac{(a+b)(a+c)(N-(a+b))(N-(a+c))}{N^{2}(N-1)}}} \\sim N(0,1)\\]\nSe rechaza la hipótesis de independencia en los extremos de la distribución normal."
  },
  {
    "objectID": "Tcontigencia.html#test-de-mcnemar",
    "href": "Tcontigencia.html#test-de-mcnemar",
    "title": "9  Tablas de Contingencia",
    "section": "9.2 Test de McNemar",
    "text": "9.2 Test de McNemar\nTable: Tabla 2 X 2.Muestras pareadas\nPEGAR TABLA DIAPO 19\n\\[X^{2}=\\frac{(b-c)^{2}}{(b+c)} \\sim \\chi_{1}^{2}\\] Se rechaza \\(H_0\\) si \\(X_{c}^{2}&gt;\\chi_{1,1-\\alpha}^{2}\\)\n\n9.2.1 Pruebas Diagnósticas\nPEGAR TABLA DIAPO 20"
  },
  {
    "objectID": "Tcontigencia.html#hipótesis-mcnemar",
    "href": "Tcontigencia.html#hipótesis-mcnemar",
    "title": "9  Tablas de Contingencia",
    "section": "9.3 Hipótesis McNemar",
    "text": "9.3 Hipótesis McNemar\n\nHipótesis Nula: Homogeneidad marginal\nHipótesis Alterna: Heterogeneidad marginal\n\n\\[\\begin{array}{l}\n\\mathrm{H}_{0}: \\frac{a}{a+c}=\\frac{a}{a+b} \\\\\n\\mathrm{H}_{a}: \\frac{a}{a+c} \\neq \\frac{a}{a+b}\n\\end{array}\\]\n\n9.3.1 Ejemplo Pruebas Diagnósticas\nPEGAR TABLA DIAPO 22"
  },
  {
    "objectID": "Tcontigencia.html#ensayos-clínicos.-sensibilidad-especificidad-y-valores-predictivos",
    "href": "Tcontigencia.html#ensayos-clínicos.-sensibilidad-especificidad-y-valores-predictivos",
    "title": "9  Tablas de Contingencia",
    "section": "9.4 Ensayos Clínicos. Sensibilidad, Especificidad y Valores Predictivos",
    "text": "9.4 Ensayos Clínicos. Sensibilidad, Especificidad y Valores Predictivos\nEn estudios clínicos:\n\nSensibilidad \\((\\pi)\\): proporción de enfermos correctamente identificados como enfermos \\(\\left(\\frac{a}{a+c}\\right)\\)\nEspecificidad \\((\\gamma)\\) : proporción de sanos correctamente identificados como sanos \\(\\left(\\frac{d}{d+b}\\right)\\)\nVPP \\(\\left(T^{+}\\right)\\): Proporción de diagnosticados como enfermos que están realmente enfermos \\(\\left(\\frac{a}{a+b}\\right)\\)\nVPN \\(\\left(T^{-}\\right)\\): Proporción de diagnosticados como sanos que están realmente sanos \\(\\left(\\frac{d}{c+d}\\right)\\)\n\n\n9.4.1 Hipótesis McNemar en Estudios Clínicos\n\nHipótesis Nula: \\(P\\) (Test positivo\\(\\mid\\)Enfermo) \\(=P\\)(Enfermo\\(\\mid\\) Test positivo Sensibilidad = VPP\nHipótesis alterna: \\(P\\)(Test positivo \\(\\mid\\)Enfermo) \\(\\neq P\\)(Enfermo\\(\\mid\\) Test positivo Sensibilidad \\(\\neq\\) VPP.\n\n\n\n9.4.2 Ejemplo Ensayo de Aptitud Química Clínica\nPEGAR TABLA DIAPO 25"
  },
  {
    "objectID": "Tcontigencia.html#ensayos-de-aptitud.-sensibilidad-especificidad-y-valores-predictivos",
    "href": "Tcontigencia.html#ensayos-de-aptitud.-sensibilidad-especificidad-y-valores-predictivos",
    "title": "9  Tablas de Contingencia",
    "section": "9.5 Ensayos de Aptitud. Sensibilidad, Especificidad y Valores Predictivos",
    "text": "9.5 Ensayos de Aptitud. Sensibilidad, Especificidad y Valores Predictivos\n\nSensibilidad: \\(P\\) (Consenso insatisfactorio/VR insatisfactorio)\nEspecificidad: \\(P\\) (Consenso satisfactorio/VR satisfactorio)\nVPP: \\(P\\) (VR insatisfactorio/Consenso insatisfactorio) \\(\\left(\\frac{a}{a+b}\\right)\\)\nVPN: \\(P\\) (VR satisfactorio/Consenso satisfactorio) \\(\\left(\\frac{d}{c+d}\\right)\\)\n\nEjemplo: Ácido Úrico.\nhttps://link.springer.com/content/pdf/10.1007/s00769-019-01423-6.pdf\n\n9.5.1 Hipótesis McNemar en Ensayos de Aptitud\n\nHipótesis Nula: \\(P\\) (Consenso InsatisfactoriojVR Insatisfactorio) = \\(P\\) (VR InsatisfactoriojConsenso Insatisfactorio) Sensibilidad = VPP\nHipótesis Alterna: \\(P\\) (Consenso Insatisfactorio\\(\\mid\\)VR Insatisfactorio) \\(\\neq\\) P(VR Insatisfactorio\\(\\mid\\)Consenso Insatisfactorio) Sensibilidad \\(\\neq\\) VPP\n\n\n\n9.5.2 Tabla de Contingencia \\(r\\) X \\(c\\)\nTable: Tabla \\(r\\) X \\(c\\) con total N fijo. 1500 pacientes de un hospital se clasifican de acuerdo a su tipo de sangre y a la severidad de una enfermedad.\nPEGAR TABLA DIAPO 28\nLa hipótesis de interés es que la severidad en la enfermedad de los pacientes es independiente de su tipo de sangre.\nTable: Tabla \\(r\\) X \\(c\\) con total N fijo o con Marginales fijos\nPEGAR TABLA DIAPO 29\nLa hipótesis de interés es que las variables A y B son independientes."
  },
  {
    "objectID": "Tcontigencia.html#test-de-independencia",
    "href": "Tcontigencia.html#test-de-independencia",
    "title": "9  Tablas de Contingencia",
    "section": "9.6 Test de Independencia",
    "text": "9.6 Test de Independencia\nDe teoría de probabilidad se sabe que si dos eventos son independientes su probabilidad conjunta es igual al producto de las probabilidades marginales. Aplicando esto a la tabla anterior se tiene:\n\\[\\begin{aligned} P_{i j} & =P_{i} P_{j}, i=1, \\ldots, r ; j=1, \\ldots, c . \\\\ E_{i j} & =N P_{i j} \\\\ & =N P_{i} P_{j} \\\\ & =N\\left(\\frac{R_{i}}{N}\\right)\\left(\\frac{C_{j}}{N}\\right) \\\\ & =\\frac{R_{i} C_{j}}{N}\\end{aligned}\\]\nTable: Tabla \\(r\\) X \\(c\\) con total N fijo o con Marginales fijos\nPEGAR TABLA DIAPO 31\nTabla con los valores esperados \\(\\left(E_{i j}=\\frac{R_{i} C_{j}}{N}\\right)\\) bajo el supuesto de independencia.\nSe puede comprobar que\n\\[X^{2}=\\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{\\left(O_{i j}-E_{i j}\\right)^{2}}{E_{i j}} \\sim \\chi_{(r-1)(c-1)}^{2}\\]\nLuego la hipótesis de independencia se rechaza la nivel \\(\\alpha\\) si\n\\[X_{c}^{2} \\geq \\chi_{(r-1)(c-1), 1-\\alpha}^{2}\\]\n\n9.6.1 Tablas de contingencia\nTable: Tabla 2 X 2con total N fijo o con Marginales fijos\nPEGAR TABLA DIAPOSITIVA 33\nLa hipótesis de interés es que las variables A y B son independientes.\n\n\n9.6.2 Test de Independencia y homogeneidad\nSe puede comprobar que\n\\[X^{2}=\\frac{N\\left(O_{11} O_{22}-O_{12} O_{21}\\right)^{2}}{R_{1} R_{2} C_{1} C_{2}} \\sim \\chi_{1}\\]\nEquivalentemente\n\\[Z=\\frac{\\sqrt{N}\\left(O_{11} O_{22}-O_{12} O_{21}\\right)}{\\sqrt{R_{1} R_{2} C_{1} C_{2}}} \\sim N(0,1)\\]\nLuego la hipótesis de independencia se rechaza la nivel \\(\\alpha\\) si\n\\[\\begin{array}{l}\nX_{c}^{2} \\geq \\chi_{1,1-\\alpha}^{2} \\\\\nZ_{c}&lt;Z_{\\frac{\\alpha}{2}} \\circ Z_{c}&gt;Z_{1-\\frac{\\alpha}{2}}\n\\end{array}\\]"
  },
  {
    "objectID": "Tcontigencia.html#corrección-de-yates",
    "href": "Tcontigencia.html#corrección-de-yates",
    "title": "9  Tablas de Contingencia",
    "section": "9.7 Corrección de Yates",
    "text": "9.7 Corrección de Yates\nSi los marginales son fijos\n\\[X^{2}=\\frac{N\\left(\\left|O_{11} O_{22}-O_{12} O_{21}\\right|-\\frac{N}{2}\\right)^{2}}{R_{1} R_{2} C_{1} C_{2}} \\sim \\chi_{1}\\]\nLuego la hipótesis de independencia se rechaza la nivel \\(\\alpha\\) si\n\\[X_{c}^{2} \\geq \\chi_{1,1-\\alpha}^{2}\\]\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Remuestreo.html#bajo-normalidad",
    "href": "Remuestreo.html#bajo-normalidad",
    "title": "10  Métodos de Remuestreo",
    "section": "10.1 Bajo Normalidad",
    "text": "10.1 Bajo Normalidad\n\n\\(Y_{1}, \\ldots, Y_{n}\\) muestra aleatoria de \\(Y \\sim N\\left(\\theta, \\sigma^{2}\\right)\\)\n\\(\\hat{\\theta}=\\bar{Y}\\) es un estimador de \\(\\theta\\)\nSesgo \\(\\begin{array}{l}\\operatorname{Sesgo}(\\hat{\\theta})=(E(\\hat{\\theta})-\\theta)=(E(\\bar{Y})-\\theta)=0 \\\\V(\\hat{\\theta})=V(\\bar{Y})=\\frac{\\sigma^{2}}{n}\\end{array}\\)\nUn IC del \\(100(1-\\alpha) \\%\\) para \\(\\theta\\) es\n\n\\[\\left((\\hat{\\theta}-\\operatorname{Sesgo}(\\hat{\\theta})) \\pm Z_{1-\\frac{\\alpha}{2}} \\sqrt{V(\\hat{\\theta})}\\right)\\]"
  },
  {
    "objectID": "Remuestreo.html#bootstrap",
    "href": "Remuestreo.html#bootstrap",
    "title": "10  Métodos de Remuestreo",
    "section": "10.2 Bootstrap",
    "text": "10.2 Bootstrap\n\n\\(Y_{1}, \\ldots, Y_{n}\\) muestra aleatoria de \\(f(y ; \\theta)\\)\n\\(\\hat{\\theta}\\) es un estimador de \\(\\theta\\)\nDistribución de \\(\\hat{\\theta}\\) desconocida\nSesgo \\(\\operatorname{Sesgo}(\\hat{\\theta})=(E(\\hat{\\theta})-\\theta)\\) desconocido\n\\(V(\\hat{\\theta})\\) desconocida\n\nTable: Muestras Bootstrap (con reemplazo): Sea \\(j = 1;... ; n\\) el tamaño de la muestra bootstrap y \\(b = 1; :: : ; B\\) el número de muestras bootstrap, \\(\\hat{\\theta}\\) es el estimador de \\(\\theta\\).\nPEGAR TABLA DIAPO 5\n\n10.2.1 Sesgo y varianza Bootstrap\n\nSesgo: Diferencia entrela media de las estimaciones del parámetro con muestras bootstrap y la estimación del parámetro con la muestra.\n\n\\[\\begin{aligned}\n\\widehat{\\operatorname{Sesgo}}(\\hat{\\theta}) & =\\hat{E}(\\hat{\\theta})-\\hat{\\theta} \\\\\n& =\\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\theta}_{b}^{*}-\\hat{\\theta}\n\\end{aligned}\\]\nVarianza: varianza de las muestras bootstrap\n\\[\\hat{V}(\\hat{\\theta})=\\frac{1}{B} \\sum_{b=1}^{B}\\left(\\hat{\\theta}_{b}^{*}-\\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\theta}_{b}^{*}\\right)^{2}\\]\n\n\n10.2.2 Intervalo de Confianza Bootstrap\n\nSe construye a partir de las estimaciones Bootstrap del sesgo y la varianza.\nSi se asume normalidad un IC del \\(100(1-\\alpha) \\%\\) para \\(\\theta\\) es\n\n\\[\\left((\\hat{\\theta}-\\widehat{\\operatorname{Sesgo}}(\\hat{\\theta})) \\pm Z_{1-\\frac{\\alpha}{2}} \\sqrt{\\hat{V}(\\hat{\\theta})}\\right)\\]\n\nSi no se asume normalidad un IC del \\(100(1-\\alpha) \\%\\) para \\(\\theta\\) es\n\n\\[\\left(\\hat{\\theta}_{\\frac{\\alpha}{2}}^{*}, \\hat{\\theta}_{1-\\frac{\\alpha}{2}}^{*}\\right)\\]\n\n\n10.2.3 Usando Bootstrap\n\nConstruir un IC del 95% para lamedianaasumiendo que tiene una muestra de tamaño 100 de una distribución \\(\\Gamma(3,2))\\)\nVer código BootstrapJackniffePermutaciones.R\nEjemplo con la mediana usando lapply\n\nhttps://stats.idre.ucla.edu/r/library/r-library-introduction-tobootstrapping/\n\n10.2.3.1 Distribución poblacional\nGRÁFICA\n\n\n10.2.3.2 Distribución de la mediana\nGRÁFICA\n\n\n10.2.3.3 Pseudo valores de Tukey\nSuponga que tiene las observaciones x1; x2; x3 y que los pseudo-valores se encuentran a partir dela media.\n\\[\\begin{aligned}\n\\tilde{x}_{1} & =n \\bar{x}-(n-1) \\bar{x}_{-1} \\\\\n& =\\frac{3\\left(x_{1}+x_{2}+x_{3}\\right)}{3}-\\frac{2\\left(x_{2}+x_{3}\\right)}{2} \\\\\n& =\\left(x_{1}+x_{2}+x_{3}\\right)-\\left(x_{2}+x_{3}\\right) \\\\\n& =x_{1} .\n\\end{aligned}\\]\nPara cualquier \\(i\\) se tiene\n\\[\\tilde{x}_{i}=x_{i}\\]\nEn general cambiando la media por otra estadística, se definen los pseudo valores de Tukey como\n\\[\\tilde{\\theta}_{i}=n \\hat{\\theta}-(n-1) \\hat{\\theta}_{-i}\\] ## Estimador Jackknife\n\\[\\begin{aligned}\n\\hat{\\theta} & =g\\left(X_{1}, \\ldots, X_{n}\\right) \\\\\n\\text { Sesgo }(\\hat{\\theta}) & =E(\\hat{\\theta})-\\theta \\quad \\text { Desconocido } \\\\\n\\hat{\\theta}_{-i} & =\\text { Estadística } \\hat{\\theta} \\text { removiendo } X_{i}, i=1, \\ldots, n \\\\\n\\hat{\\theta}_{\\mathrm{n}} & =\\frac{1}{n} \\sum_{i=1}^{n} \\hat{\\theta}_{-i} \\\\\n\\tilde{\\theta}_{i} & =n \\hat{\\theta}-(n-1) \\hat{\\theta}_{-i} \\quad \\text { Pseudo valores de Tukey } \\\\\n\\hat{\\theta}_{\\text {jack }} & =\\frac{1}{n} \\sum_{i=1}^{n} \\tilde{\\theta}_{i} \\quad \\text { Promedio de pseudo valores }\n\\end{aligned}\\]\n\n\n\n10.2.4 Estimador Jackknife: Ejemplo de la Varianza\n\\[\\hat{\\theta}=S^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}\\]\nSesgo\n\\[\\begin{aligned}\n\\left(S^{2}\\right) & =E\\left(S^{2}\\right)-\\sigma^{2}=\\left(\\frac{n-1}{n} \\sigma^{2}\\right)-\\sigma^{2} \\\\\nS_{-i}^{2} & =\\text { Estadística } S^{2} \\text { removiendo } X_{i}, i=1, \\ldots, n \\\\\nS_{n}^{2} & =\\frac{1}{n} \\sum_{i=1}^{n} S_{-i}^{2} \\\\\n\\tilde{S}_{i}^{2} & =n S^{2}-(n-1) S_{-i}^{2} \\quad \\text { Pseudo valores de Tukey } \\\\\n\\hat{\\theta}_{\\text {jack }} & =\\frac{1}{n} \\sum_{i=1}^{n} \\tilde{S}_{i}^{2}\n\\end{aligned}\\]"
  },
  {
    "objectID": "Remuestreo.html#sesgo-jackknife",
    "href": "Remuestreo.html#sesgo-jackknife",
    "title": "10  Métodos de Remuestreo",
    "section": "10.3 Sesgo Jackknife",
    "text": "10.3 Sesgo Jackknife\n\\[\\begin{aligned} \\hat{\\theta}_{\\text {jack }} & =\\frac{1}{n} \\sum_{i=1}^{n} \\tilde{\\theta}_{i} \\quad \\text { Promedio de pseudo valores } \\\\ & =\\frac{1}{n} \\sum_{i=1}^{n}\\left(n \\hat{\\theta}-(n-1) \\hat{\\theta}_{-i}\\right) \\\\ & =\\frac{1}{n} \\sum_{i=1}^{n} n \\hat{\\theta}-(n-1) \\frac{1}{n} \\sum_{i=1}^{n} \\hat{\\theta}_{-i} \\\\ & =n \\hat{\\theta}-(n-1) \\hat{\\theta}_{n}\\end{aligned}\\]\nEn general: Sesgo \\((\\hat{\\theta})=(E(\\hat{\\theta})-\\theta)\\). En jackknife para estimar el sesgo se reemplaza \\(E(\\hat{\\theta})\\) por \\(\\hat{\\theta}\\) y \\(\\theta\\) o por \\(\\hat{\\theta}_{\\text {jack }}\\)\nSesgo jack= \\[\\begin{array}{l}\n=\\hat{\\theta}-\\left(n \\hat{\\theta}-(n-1) \\hat{\\theta}_{n}\\right) \\\\\n=\\hat{\\theta}-n \\hat{\\theta}+(n-1) \\hat{\\theta}_{n} \\\\\n=(n-1) \\hat{\\theta}_{n}-(n-1) \\hat{\\theta} \\\\\n=(n-1)\\left(\\hat{\\theta}_{n}-\\hat{\\theta}\\right) \\\\\n\\end{array}\\]\n\n10.3.1 Estimador Jackknife a partir del sesgo\n\\[\\begin{array}{l}=\\hat{\\theta}-(n-1)\\left(\\hat{\\theta}_{n}-\\hat{\\theta}\\right) \\\\ =\\hat{\\theta}-(n-1) \\hat{\\theta}_{n}+(n-1) \\hat{\\theta} \\\\ =\\hat{\\theta}-n \\hat{\\theta}_{n}+\\hat{\\theta}_{n}+n \\hat{\\theta}-\\hat{\\theta} \\\\ =\\hat{\\theta}_{n}-n \\hat{\\theta}_{n}+n \\hat{\\theta} \\\\ =n \\hat{\\theta}-n \\hat{\\theta}_{n}+\\hat{\\theta}_{n} \\\\ =n \\hat{\\theta}-(n-1) \\hat{\\theta}_{n} \\\\\\end{array}\\]\n\n\n10.3.2 Propiedades de la varianza\nSea \\(X_{1}, \\ldots, X_{n}\\) una muestra aleatoria y \\(\\bar{X}\\) la media muestral.\nEntonces\n\\[\\begin{array}{l}\nV(\\bar{X})=\\frac{\\sigma^{2}}{n} \\rightarrow \\hat{V}(\\bar{X})=\\frac{S^{2}}{n} \\\\\n\\hat{V}(\\bar{X})=\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}\n\\end{array}\\]\nUsando los pseudo-valores \\(\\tilde{\\theta}_{1}, \\ldots, \\tilde{\\theta}_{n}\\)\n\\[\\hat{V}\\left(\\hat{\\theta}_{\\text {jack }}\\right)=\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left(\\tilde{\\theta}_{i}-\\frac{1}{n} \\sum_{j=1}^{n} \\tilde{\\theta}_{j}\\right)^{2}\\]\nShao, J. and Tu, D. 1985. The Jackknife and Bootstrap. Springer. Páginas 5 y 6.\n\n\n10.3.3 Ejemplo de juguete\n\\[\\begin{aligned} \\hat{\\theta} & =\\bar{X} . n=3 . \\\\ x_{1} & =2, x_{2}=3, x_{3}=4, \\bar{x}=3 \\\\ \\hat{\\theta} & =3, \\\\ \\hat{\\theta}_{-1} & =\\frac{1}{2}\\left(x_{2}+x_{3}\\right)=3.5, \\\\ \\hat{\\theta}_{-2} & =\\frac{1}{2}\\left(x_{1}+x_{3}\\right)=3.0, \\\\ \\hat{\\theta}_{-3} & =\\frac{1}{2}\\left(x_{1}+x_{2}\\right)=2.5, \\\\ \\hat{\\theta}_{n} & =\\frac{1}{3} \\sum_{i=1}^{3} \\theta_{-i}=3 .\\end{aligned}\\]\n\\[\\begin{aligned} \\tilde{\\theta}_{1} & =3 \\hat{\\theta}-2\\left(\\hat{\\theta}_{-1}\\right)=9-7=2 \\\\ \\tilde{\\theta}_{2} & =3 \\hat{\\theta}-2\\left(\\hat{\\theta}_{-2}\\right)=9-6=3 \\\\ \\tilde{\\theta}_{3} & =3 \\hat{\\theta}-2\\left(\\hat{\\theta}_{-3}\\right)=9-5=4 \\\\ \\hat{\\theta}_{\\text {jack }} & =\\frac{1}{3} \\sum_{i=1}^{3} \\tilde{\\theta}_{-i}=\\frac{1}{3}(2+3+4)=3 \\\\ \\hat{\\theta}_{\\text {jack }} & =n(\\hat{\\theta})-(n-1) \\hat{\\theta}_{n}=3(3)-2(3)=3 \\\\ V\\left(\\hat{\\theta}_{\\text {jack }}\\right) & =\\frac{1}{6}\\left((2-3)^{2}+(3-3)^{2}+(4-3)^{2}\\right) \\\\ & =\\frac{1}{6}\\left(1^{2}+1^{2}\\right)=\\frac{1}{3} \\\\ \\hat{\\theta} & =\\hat{\\theta}_{n}=\\hat{\\theta}_{\\text {jack. }} \\text { Por qué? }\\end{aligned}\\]\n\n\n10.3.4 Distribución del estimador Jackknife\n\\[T=\\frac{\\hat{\\theta}_{\\text {jack }}-\\theta}{\\sqrt{\\hat{V}\\left(\\hat{\\theta}_{\\text {jack }}\\right)}} \\sim \\text { T-student }_{n-1}\\]\nMiller, R. 1974. The Jackknife-A Review. Biometrika, 61 (1), 1-15\n\n\n10.3.5 IC Jackknife\nUn IC de confianza del \\(100(1-\\alpha)\\) para \\(\\theta\\) es\n\\[\\begin{array}{l}\n\\left(\\hat{\\theta}_{\\text {jack }} \\pm t_{n-1,1-\\frac{\\alpha}{2}} \\sqrt{\\hat{V}\\left(\\hat{\\theta}_{\\text {jack }}\\right)}\\right) \\\\\n\\left(\\left(\\hat{\\theta}-\\text { Sesgo }_{\\text {jack }}\\right) \\pm t_{n-1,1-\\frac{\\alpha}{2}} \\sqrt{\\hat{V}\\left(\\hat{\\theta}_{\\text {jack }}\\right)}\\right)\n\\end{array}\\]\nEstimating Uncertainty in Population Growth Rates: Jackknife vs. Bootstrap Techniques. Meyer, J., Ingersoll, C., McDonald, L and Boyce, M. 1986. Ecology, 67 (5), 1156-1166\n\n\n10.3.6 Jackknife. Ejemplo con CV\n\\[\\begin{aligned} \\hat{C} & =g\\left(X_{1}, \\ldots, X_{n}\\right): C V \\text { de la muestra } \\\\ \\hat{C}_{-i} & =C V \\text { de }\\left(X_{1}, \\ldots, X_{i-1}, X_{i+1}, \\ldots, X_{n}\\right) \\\\ \\hat{C}_{n} & =\\frac{1}{n} \\sum_{i=1}^{n} \\hat{C}_{-i}: \\text { Promedio de } C V \\\\ \\hat{C}_{\\text {jack }} & =n \\hat{C}-(n-1) \\hat{C}_{n} \\\\ V\\left(\\hat{C}_{\\text {jack }}\\right) & =\\frac{(n-1)}{n} \\sum_{i=1}^{n}\\left(\\hat{C}_{-i}-\\hat{C}_{n}\\right)^{2} \\text { Fórmula alterna } \\\\ \\text { Sesgo jack } & =(n-1)\\left(\\hat{C}_{n}-\\hat{C}\\right) \\\\ I C & =\\left(\\left(\\hat{C}-\\text { Sesgo }_{\\text {jack }}\\right) \\pm t_{n-1,1-\\frac{\\alpha}{2}} \\sqrt{V\\left(\\hat{C}_{\\text {jack }}\\right)}\\right)\\end{aligned}\\]\n\n\n10.3.7 Fórmula alterna de la varianza\nPuede demostrarse que\n\\[\\begin{array}{l}\nV\\left(\\hat{\\theta}_{\\text {jack }}\\right)=\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left(\\tilde{\\theta}_{i}-\\frac{1}{n} \\sum_{j=1}^{n} \\tilde{\\theta}_{j}\\right)^{2} \\text { es igual a } \\\\\nV\\left(\\hat{\\theta}_{\\text {jack }}\\right)=\\frac{(n-1)}{n} \\sum_{i=1}^{n}\\left(\\hat{\\theta}_{-i}-\\hat{\\theta}_{n}\\right)^{2} \\text { ver prueba }\n\\end{array}\\]\nEn el caso del coeficiente de variación se tiene\n\\[\\begin{array}{l}\nV\\left(\\hat{C}_{\\text {jack }}\\right)=\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left(\\tilde{C}_{i}-\\frac{1}{n} \\sum_{j=1}^{n} \\tilde{C}_{j}\\right)^{2} \\text { es igual a } \\\\\nV\\left(\\hat{C}_{\\text {jack }}\\right)=\\frac{(n-1)}{n} \\sum_{i=1}^{n}\\left(\\hat{C}_{-i}-\\hat{C}_{n}\\right)^{2}\n\\end{array}\\]\n\\[\\begin{aligned} V\\left(\\hat{\\theta}_{\\mathrm{jack}}\\right) & =\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left(\\tilde{\\theta}_{i}-\\frac{1}{n} \\sum_{j=1}^{n} \\tilde{\\theta}_{j}\\right)^{2} \\\\ & =\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left[n \\hat{\\theta}-(n-1) \\hat{\\theta}_{-i}-\\frac{1}{n} \\sum_{j=1}^{n}\\left(n \\hat{\\theta}-(n-1) \\hat{\\theta}_{-j}\\right)\\right]^{2} \\\\ & =\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left[n \\hat{\\theta}-(n-1) \\hat{\\theta}_{-i}-\\frac{n^{2} \\hat{\\theta}}{n}+\\frac{(n-1)}{n} \\sum_{j=1}^{n} \\hat{\\theta}_{-j}\\right]^{2} \\\\ & =\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left[n \\hat{\\theta}-(n-1) \\hat{\\theta}_{-i}-n \\hat{\\theta}+(n-1) \\hat{\\theta}_{n}\\right]^{2}\\end{aligned}\\]\n\\[\\begin{array}{l}=\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left[(n-1) \\hat{\\theta}_{n}-(n-1) \\hat{\\theta}_{-i}\\right]^{2} \\\\ =\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left[(n-1)\\left(\\hat{\\theta}_{n}-\\hat{\\theta}_{-i}\\right)\\right]^{2} \\\\ =\\frac{1}{n(n-1)} \\sum_{i=1}^{n}\\left[(n-1)^{2}\\left(\\hat{\\theta}_{n}-\\hat{\\theta}_{-i}\\right)^{2}\\right] \\\\ =\\frac{(n-1)^{2}}{n(n-1)} \\sum_{i=1}^{n}\\left(\\hat{\\theta}_{n}-\\hat{\\theta}_{-i}\\right)^{2} \\\\ =\\frac{(n-1)}{n} \\sum_{i=1}^{n}\\left(\\hat{\\theta}_{-i}-\\hat{\\theta}_{n}\\right)^{2}\\end{array}\\]\n\n\n10.3.8 Jackknife\nAGREGAR TABLA DIAPO 26\n\n\n10.3.9 Usando Jackknife\n\nConstruir un IC del 95% para el coeficiente de variaciÓn asumiendo que tiene una muestra de tamaño 100 de una distribución \\(\\Gamma(3,2))\\)\nVer códigoBootstrapJackniffePermutaciones.R\n\n\n10.3.9.1 Distribución del CV\nGRÁFICA DIAPO 28\n\n\n\n10.3.10 Permutaciones\n\nSe emplea para estudiar de diferencias entre grupos.\nSe define una estadística de prueba \\((T)\\) y se calcula con la configuración original de datos \\(T_{c}\\).\nSe hacen \\(k\\) permutaciones.En cada iteración se calcula \\(T\\). Con base en \\(\\left(T_{1}, T_{2}, \\ldots, T_{k}\\right)\\) se establece la distribución.\nSi \\(T_{c}\\) está en los extremos de la distribución se rechaza la hipótesis de igualdad entre los grupos.\nEjemplo: Distribución de \\(\\left|\\tilde{X}_{1}-\\tilde{X}_{2}\\right|\\)\nVer código BootstrapJackniffePermutaciones.R\n\n\n10.3.10.1 Test de permutaciones\nGRÁFICA DIAPO 30\nGRÁFICA DIAPO 31\nGRÁFICA DIAPO 32\nGRÁFICA DIAPO 33"
  },
  {
    "objectID": "Remuestreo.html#método-para-generar-muestras-aleatorias",
    "href": "Remuestreo.html#método-para-generar-muestras-aleatorias",
    "title": "10  Métodos de Remuestreo",
    "section": "10.4 Método para generar muestras aleatorias",
    "text": "10.4 Método para generar muestras aleatorias\n\nSe basa en el uso de valores pseudoaletorios uniformes \\(u\\) (los softwares traen algoritmos para obtenerlos: Middle-square, generadores congruenciales, Mersene-Twister,..)\nSuponga que \\(X \\sim f(x ; \\theta)\\)\n\\(F(x ; \\theta) \\in[0,1]\\)\nSe genera un valor aleatorio uniforme \\(u \\in[0,1]\\)\nSe iguala la función de distribución al valor uniforme simulado y se despeja \\(x\\)\nEjercicio: Cómo generar valores aleatorios de una exponencial de parámetro \\(\\theta\\)"
  },
  {
    "objectID": "Remuestreo.html#simulaciones-de-monte-carlo",
    "href": "Remuestreo.html#simulaciones-de-monte-carlo",
    "title": "10  Métodos de Remuestreo",
    "section": "10.5 Simulaciones de Monte Carlo",
    "text": "10.5 Simulaciones de Monte Carlo\n\nSuponga que \\(X \\sim f(x ; \\theta)\\)\nSe define una estadística \\(T_{n}\\)\nLa distribución de la estadística se obtiene calculándola en muchas muestras aleatorias de tamaño \\(N\\) de \\(X \\sim f(x ; \\theta)\\)\nSe pueden encontrar intervalos de confianza y hacer pruebas de hipótesis a partir de las simulaciones.\nEjemplo: Distribución de la varianza \\(S^{2}\\) si \\(X \\sim \\Gamma(\\alpha=3, \\beta=2)\\)\n\n\n10.5.1 Monte Carlo\nGRÁFICA DIAPO 36"
  },
  {
    "objectID": "Remuestreo.html#validación-cruzada",
    "href": "Remuestreo.html#validación-cruzada",
    "title": "10  Métodos de Remuestreo",
    "section": "10.6 Validación Cruzada",
    "text": "10.6 Validación Cruzada\n\nSuponga que tiene \\(y_{1}, \\ldots, y_{n}\\)\nQuita \\(y_{i}\\), estima el modelo y obtine \\(\\hat{y}_{i}\\) (leave-one-out)\n\\(S C E=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\\). Sirve para comparar modelos\nPuede hacerse dividiendo la muestra en dos partes (two-fold). Muestra de entrenamiento y de prueba (training and test)\nEjemplo 1: Aplicación en Regresión http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/\nEjemplo 2. Aplicación en redes Neuronales https://www.analyticsvidhya.com/blog/2017/09/creatingvisualizing-neural-network-in-r/"
  },
  {
    "objectID": "Remuestreo.html#regresión-lineal-y-redes-neuronales",
    "href": "Remuestreo.html#regresión-lineal-y-redes-neuronales",
    "title": "10  Métodos de Remuestreo",
    "section": "10.7 Regresión Lineal y Redes Neuronales",
    "text": "10.7 Regresión Lineal y Redes Neuronales\nAGREGAR TABLA DIAPO 39\n\n10.7.1 Gráfico Red Neuronal Datos Cereales\nGRÁFICO DIAPO 40\n\n1 + 1\n\n[1] 2"
  }
]