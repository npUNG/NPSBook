[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estadistica No Parametrica",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1*6\n\n[1] 7"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "signo.html#tipos-de-variables-y-escalas-de-medida",
    "href": "signo.html#tipos-de-variables-y-escalas-de-medida",
    "title": "3  Test del Signo",
    "section": "3.1 Tipos de variables y escalas de medida",
    "text": "3.1 Tipos de variables y escalas de medida\n\n3.1.1 Cualitativas\nEscala Nominal: Se presenta cuando los datos no pueden ser ordenados en una secuencia de grados del atributo. p. ej. épocas climáticas (seco, lluvioso), género (hombre, mujer) sitios geográficos (costa, interior llanos, sur), color (rojo, verde, amarillo), marca (renault, mazda, chevrolet), religión (católico, protestante, judaísmo, islam).\nEscala Ordinal: Los datos que pueden ser ordenados de menor a mayor o viceversa, pero las distancias entre los elementos ordenados no tienen ningún sentido físico. Ej: Estrato (I, II,…,VI), Abundancia (abundante, escaso, raro), calidad (buena, mala, regular),talla (grande, mediano, pequeño).\nLos valores ordinales no pueden someterse a operaciones matemáticas como la adición; por eso, no es posible afirmar que el nivel abundante es tres veces mayor que el estado raro.\n\n\n3.1.2 Cuantitativas\nCantidades, que son el resultado de mediciones de algún instrumento, conteos de eventos o de operaciones matemáticas simples. Estos pueden ser\n\nDiscretas: Pueden tomar solo un número finito de posibles valores en la escala de los reales. Ej: número de habitaciones por vivienda, número de hijos por familia, número de peces por estanque, número de temblores por región, número de pacientes por hospital.\nContinuas: Son aquellos en los que existe potencialmente un número infinito de valores entre dos puntos de la escala. Pueden ser datos enteros o fraccionarios, p. ej. Ingreso (pesos), temperatura (grados celsius), peso (kg), tensión arterial (mmHg).\nDerivadas: Son datos generados a partir de cálculos simples de las medidas discretas o continuas, p. ej. índices, tasas, proporciones, etc."
  },
  {
    "objectID": "signo.html#razón-proporción-tasa",
    "href": "signo.html#razón-proporción-tasa",
    "title": "3  Test del Signo",
    "section": "3.2 Razón, Proporción, Tasa",
    "text": "3.2 Razón, Proporción, Tasa\n\nRazón: El numerador no forma parte del denominador. Ej. Número de Personas del Hogar/Número de habitaciones, Número de pacientes/Número de camas, Número de diabéticos/Número de no diabéticos, Número de hombres/Número de mujeres, índice de peso-talla=Kg/(cms-100).\nProporción:El numerador hace parte del denominador. Ej: pacientes recuperados/pacientes tratados, alumnos que aprobaron el examen/ alumnos que presentaron el examen, peso cabeza/peso corporal (no necesariamente conteos!)\nTasa: razón o proporción en la que se define un tiempo de ocurrencia. Ej: a) nacidos vivos periodo /mujeres entre 15 y 50 años en el periodo, b) casos en el periodo/población en riesgo en el periodo."
  },
  {
    "objectID": "signo.html#introducción",
    "href": "signo.html#introducción",
    "title": "3  Test del Signo",
    "section": "3.3 Introducción",
    "text": "3.3 Introducción\nMétodos estadísticos no paramétricos:\n\nTienen supuestos distribucionales más flexibles. Las pruebas clásicas (una muestra, dos muestras, k muestras) asumen normalidad. Hay alternativas no paramétricas libres de este supuesto.\n\nHay técnicas apropiadas para el estudio de variables con escala nominal y ordinal."
  },
  {
    "objectID": "signo.html#mediana",
    "href": "signo.html#mediana",
    "title": "3  Test del Signo",
    "section": "3.4 Mediana",
    "text": "3.4 Mediana\n\nPoblación: Sea \\(X\\) una variable aleatoria con distribución \\(F_x(X)\\). La mediana \\((\\theta)\\) es un valor tal que \\(F_x(\\theta)=\\frac{1}{2}\\) (si \\(X\\) es continua) o \\(P(x\\leq\\theta)\\geq\\frac{1}{2}\\) y \\(P(x\\geq\\theta)\\geq\\frac{1}{2}\\) (si \\(X\\) es discreta).\nSea \\(X\\) ~ exponencial(\\(\\lambda=3\\)). Halle la mediana.\nSea \\(X\\) ~ binomial \\((n = 9; p =\\frac{1}{4})\\). Compruebe que la mediana es 2."
  },
  {
    "objectID": "signo.html#mediana-no-única",
    "href": "signo.html#mediana-no-única",
    "title": "3  Test del Signo",
    "section": "3.5 Mediana No Única",
    "text": "3.5 Mediana No Única\n\nPoblación: Sea \\(X\\) ~ binomial \\((n=21; p=0.5)\\)\n\\(P(X\\leq10)=0.5\\) y \\(P(X\\geq10)=0.67\\). Entonces \\(X=10\\) es una mediana.\n\\(P(X\\leq11)=0.67\\) y \\(P(X\\geq11)=0.5\\). Entonces \\(X=11\\) es una mediana.\n\\(X\\) ~ binomial \\((n = 21; p =0.5)\\) la mediana no es única.\n\n\n3.5.1 Test del Signo\nPoblación: Sea \\(X\\) una variable aleatoria cuya distribución tiene mediana \\(\\theta\\), es decir \\(\\theta\\) es un valor tal que \\(F_x(\\theta)=\\frac{1}{2}\\) Muestra: \\(X_1\\); \\(X_2\\);…; \\(X_n\\) una muestra aleatoria de \\(X\\).\nHipótesis:\n\\[\\begin{aligned} H_{0}: \\theta & =\\theta_{0} \\\\ H_{a}: \\theta & \\neq \\theta_{0} \\\\ \\theta & &gt;\\theta_{0} \\\\ \\theta & &lt;\\theta_{0}\\end{aligned}\\] Sea \\(Y_1=X_1-\\theta_0...;Y_n=X_n-\\theta_0\\). Entonces las hipótesis pueden plantearse como\n\\[\\begin{aligned} H_{0}: \\theta & =0 \\\\ H_{a}: \\theta & \\neq 0 \\\\ \\theta & &gt;0 \\\\ \\theta & &lt;0\\end{aligned}\\] Es claro que \\(\\theta\\) representa la mediana de \\(Y\\).\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}1 & \\text { if } Y_{i}&gt;0 \\\\ 0 & \\text { if } Y_{i}&lt;0\\end{array}\\right.\\]\nBajo \\(H_{0}, P\\left(\\psi_{i}=1\\right)=P\\left(Y_{i}&gt;0\\right)=\\frac{1}{2}\\)\n\\[S=\\sum_{i=1}^{n} \\psi_{i} \\sim \\operatorname{Bin}\\left(n, \\frac{1}{2}\\right)\\]\n\nEstadística de prueba y región de rechazo. \\(H_a:\\theta&gt;0\\)\n\nSe rechaza \\(H_0\\) si \\(S \\geq b_{(1-\\alpha, 1 / 2)}\\) donde \\(b_{(1-\\alpha, 1 / 2)}\\) es el percentil \\(1-\\alpha\\) de la distribución binomial \\(n,p=\\frac{1}{2}\\).\n\n# Una cola superior: Se rechaza la nivel alpha=6\\% si Sc&gt;11\nn=15\np=.5\nx=seq(0, n, by=1)\ncbind(x,round(pbinom(x-1, n, p, lower.tail=FALSE) ,2))\n\n       x     \n [1,]  0 1.00\n [2,]  1 1.00\n [3,]  2 1.00\n [4,]  3 1.00\n [5,]  4 0.98\n [6,]  5 0.94\n [7,]  6 0.85\n [8,]  7 0.70\n [9,]  8 0.50\n[10,]  9 0.30\n[11,] 10 0.15\n[12,] 11 0.06\n[13,] 12 0.02\n[14,] 13 0.00\n[15,] 14 0.00\n[16,] 15 0.00\n\n\n\nEstadística de prueba y región de rechazo. \\(H_a:\\theta&lt;0\\)\n\nSe rechaza \\(H_0\\) si \\(S \\leq n-b_{(1-\\alpha, 1 / 2)}\\), donde \\(b_{(1-\\alpha, 1 / 2)}\\) es el percentil \\(1-\\alpha\\) de la distribución binomial \\(n,p=\\frac{1}{2}\\)\n\n# Una cola inferior: Se rechaza al nivel alpha=6\\% si Sc&lt;=4\nn=15\np=.5\nx=seq(0, n, by=1)\ncbind(x,round(pbinom(x, n, p) ,2))\n\n       x     \n [1,]  0 0.00\n [2,]  1 0.00\n [3,]  2 0.00\n [4,]  3 0.02\n [5,]  4 0.06\n [6,]  5 0.15\n [7,]  6 0.30\n [8,]  7 0.50\n [9,]  8 0.70\n[10,]  9 0.85\n[11,] 10 0.94\n[12,] 11 0.98\n[13,] 12 1.00\n[14,] 13 1.00\n[15,] 14 1.00\n[16,] 15 1.00\n\n\n\nEstadística de prueba y región de rechazo. \\(H_a:\\theta \\neq 0\\)\n\nSe rechaza \\(H_0\\) si \\(S \\leq n-b_{(1-\\alpha /2, 1 / 2)}\\) o \\(S \\geq b_{(1-\\alpha/2, 1 / 2)}\\), donde \\(b_{(1-\\alpha/2, 1 / 2)}\\) es el percentil \\(1-\\alpha/2\\) de la distribución binomial \\(n,p=\\frac{1}{2}\\).\n\n# Dos colas: Se rechaza la nivel alpha=12\\% si Sc&gt;=11 o Sc&lt;=4 \nn=15\np=.5\nx=seq(0, n, by=1)\ncbind(x,round(pbinom(x, n, p) ,2), round(pbinom(x-1, n, p, lower.tail=FALSE) ,2))\n\n       x          \n [1,]  0 0.00 1.00\n [2,]  1 0.00 1.00\n [3,]  2 0.00 1.00\n [4,]  3 0.02 1.00\n [5,]  4 0.06 0.98\n [6,]  5 0.15 0.94\n [7,]  6 0.30 0.85\n [8,]  7 0.50 0.70\n [9,]  8 0.70 0.50\n[10,]  9 0.85 0.30\n[11,] 10 0.94 0.15\n[12,] 11 0.98 0.06\n[13,] 12 1.00 0.02\n[14,] 13 1.00 0.00\n[15,] 14 1.00 0.00\n[16,] 15 1.00 0.00\n\n\n\n# alpha=10% un cola inferior\n# Se rechaza $H_0$ si $S\\geq b_{(\\alpha, 1/2)}$, \n# donde $b_{(\\alpha, 1/2)}$ es el percentil $\\alpha$ \n# de la distribuci?n binomial $(n, p=1/2)$"
  },
  {
    "objectID": "signo.html#resultados-asintóticos",
    "href": "signo.html#resultados-asintóticos",
    "title": "3  Test del Signo",
    "section": "3.6 Resultados asintóticos",
    "text": "3.6 Resultados asintóticos\n\n\\(X_1,...X_n\\) m.a de \\(X\\) con distribución \\(P(X \\leq x)\\)\nAsintóticamente \\(\\sum_{i=1}^{n} X_{i} \\sim N\\left(E\\left(\\sum_{i=1}^{n} X_{i}\\right), V\\left(\\sum_{i=1}^{n} X_{i}\\right)\\right)\\)\nAsintóticamente \\(Z=\\left(\\frac{\\sum_{i=1}^{n} x_{i}-E\\left(\\sum_{i=1}^{n} x_{i}\\right)}{\\sqrt{V\\left(\\sum_{i=1}^{n} x_{i}\\right)}}\\right) \\sim N(0,1)\\)\nAsintóticamente \\(\\bar{X} \\sim N(\\mathbb{E}(\\bar{X}), \\mathbb{V}(\\bar{X}))\\)\nAsintóticamente \\(Z=\\left(\\frac{\\bar{X}-\\mathbb{E}(\\bar{X})}{\\sqrt{\\mathbb{V}(\\bar{X})}}\\right) \\sim N(0,1)\\)\n\\(\\psi_{1},...\\psi_{n}\\) m.a. con \\(\\psi_{i}\\) ~ Bernoulli (1/2)\nAsintóticamente \\(S=\\sum_{i=1}^{n} \\psi_{i} \\sim N\\left(\\frac{n}{2}, \\frac{n}{4}\\right)\\)\nAsintóticamente \\(Z=\\left(\\frac{S-\\left(\\frac{n}{2}\\right)}{\\sqrt{\\left(\\frac{n}{4}\\right)}}\\right) \\sim N(0,1)\\)\nAsintóticamente \\(\\frac{S}{n} \\sim N\\left(\\left(\\frac{1}{2}\\right),\\left(\\frac{1}{4 n}\\right)\\right)\\)\nAsintóticamente \\(Z=\\left(\\frac{\\frac{S}{n}-\\left(\\frac{1}{2}\\right)}{\\sqrt{\\left(\\frac{1}{4 n}\\right)}}\\right) \\sim N(0,1)\\)"
  },
  {
    "objectID": "signo.html#test-del-signo-1",
    "href": "signo.html#test-del-signo-1",
    "title": "3  Test del Signo",
    "section": "3.7 Test del signo",
    "text": "3.7 Test del signo\nUsando la Distribución Asintóticapara el caso de \\(H_a:\\theta&gt;0\\). Encontrar, empleando la distribución asintótica, el valor de \\(k\\) tal que \\(P(S\\geq k)=\\alpha\\)\n\\[Z=\\frac{S-\\frac{n}{2}}{\\sqrt{\\frac{n}{4}}} \\sim N(0,1)\\] \\[\\begin{aligned} P(S \\geq k) & =P\\left(\\frac{S-\\frac{n}{2}}{\\sqrt{\\frac{n}{4}}}&gt;\\frac{k-\\frac{n}{2}}{\\sqrt{\\frac{n}{4}}}\\right) \\\\ & =P\\left(Z&gt;\\frac{k-\\frac{n}{2}}{\\sqrt{\\frac{n}{4}}}\\right) \\\\ & =P\\left(Z&gt;Z_{1-\\alpha}\\right) \\\\ k & =\\frac{n}{2}+Z_{1-\\alpha} \\sqrt{\\frac{n}{4}}\\end{aligned}\\] Ejemplo:El porcentaje de bacterias en muestras de agua de desecho no debe ser superior al 40%. La observación de 10 “muestras” de agua de desecho arrojó los siguientes porcentajes:\n\\[\\begin{array}{r}\nx_{i}=41,33,43,52,46,37,44,49,53,30 . \\\\\nH_{0}: \\theta=40 \\\\\nH_{a}: \\theta&gt;40\n\\end{array}\\]\nSe obtienen los datos:\n\\[\\begin{array}{l}\ny_{i}=\\left(x_{i}-40\\right)=1,-7,3,12,3,-3,4,9,13,-10 . \\\\\n\\psi_{i}=1,0,1,1,1,0,1,1,1,0 . \\\\\nS_{c}=\\sum_{i=1}^{n} \\psi_{i}=7 . \\quad P(S \\geq 7)=0.172 \\quad S \\sim \\operatorname{Bin}(10,1 / 2) .\n\\end{array}\\]\nsi \\(\\alpha=5 \\%\\) entonces se rechaza \\(H_0\\)"
  },
  {
    "objectID": "signo.html#test-de-wilcoxon",
    "href": "signo.html#test-de-wilcoxon",
    "title": "3  Test del Signo",
    "section": "3.8 Test de Wilcoxon",
    "text": "3.8 Test de Wilcoxon\nPoblación: \\(X\\) una variable aleatoria con distribución simétrica y continua de mediana \\(\\theta\\) (Hollander, M. and Wolfe, D. 1999. Nonparametric Statistical Methods. John Wiley & Sons)\nDistribuciones simétricas continuas: Weibull, Gamma, Beta, Uniforme, Triangular, etc.\nMuestra: \\(X_1, X_2,...X_n\\) una muestra aleatoria de \\(X\\).\nHipótesis\n\\[\\begin{aligned} H_{0}: & =\\theta_{0} \\\\ H_{a}: \\theta & \\neq \\theta_{0} \\\\ \\theta & &gt;\\theta_{0} \\\\ \\theta & &lt;\\theta_{0}\\end{aligned}\\] \\(Y_{1}=X_{1}-\\theta_{0}, Y_{2}=X_{2}-\\theta_{0}, \\ldots, Y_{n}=X_{n}-\\theta_{0}\\) una muestra aleatoria de \\(X\\)\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i}&gt;0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\n\\(R_{i}^{+}=\\) posición que ocupa \\(\\left|Y_{i}\\right|\\) en la sucesión ordenada \\[|Y|_{(1)} \\leq|Y|_{(2)} \\leq \\cdots \\leq|Y|_{(n)}\\] Estadística de prueba:\n\\[T=\\sum_{i=1}^{n} R_{i}^{+} \\psi_{i}\\] ver Tabla A.4 Hollander & Wolfe para valores críticos. Se puede demostrar (ejercicio) que cuando \\(n\\) es grande\n\\[\\begin{array}{c}\nE(T)=\\frac{n(n+1)}{4} \\\\\nV(T)=\\frac{n(n+1)(2 n+1)}{24}\n\\end{array}\\]\nDistribución Asintótica: Para \\(n\\) grande\n\\[Z=\\frac{T-E(T)}{\\sqrt{V(T)}} \\sim N(0,1)\\]\nRegión de rechazo\nSe rechaza \\(H_0\\) si \\(T\\geq k\\), con \\(k\\) tal que \\(P(T\\geq k) =\\alpha\\)\n\\[k=\\frac{n(n+1)}{4}+Z_{1-\\alpha} \\sqrt{\\frac{n(n+1)(2 n+1)}{24}}\\] Ejemplo: En un grupo de 12 personas se mide el cambio en el ritmo cardíaco (latidos/minuto) después de levantarse. \\(H_0 : \\theta = 15\\).\nPEGAR TABLA DE DIAPOSITIVA 25\n\\[\\begin{aligned}\nT_{c} & =\\sum R_{i}^{+} S\\left(y_{i}\\right)=14 \\\\\nE(T) & =\\frac{n(n+1)}{4}=39 \\\\\nV(T) & =\\frac{n(n+1)(2 n+1)}{24}=162.5 \\\\\nZ_{c} & =\\frac{T_{c}-E(T)}{\\sqrt{V(T)}}=-1.9611\n\\end{aligned}\\]\nSe rechaza \\(H_{0}\\) al 5%, si \\(Z_{c}&gt;Z_{1-\\alpha}=1.645\\) Luego, no hay evidencia para rechazar \\(H_{0}\\).\n\nDistribución exacta: Se construye generando el conjunto de todos los arreglos posibles (formas como pueden aparecer observaciones positivas y negativas en los en la muestra y obteniendo todos los posibles valores de T.\nValores críticos: Ver tablas en (Hollander, M. and Wolfe, D. 1999. Nonparametric Statistical Methods. John Wiley & Sons)\nCeros: Si hay ceros en los \\(Y_i\\), se descartan y se toma \\(n\\) como el número de valores distintos de cero.\nEmpates: Si hay \\(|Y_i|, R_i^+\\) empates en los corresponde al promedio de los rangos correspondientes.\n\nEjemplo cuando hay empates: En un grupo de 12 personas se mide el cambio en el ritmo cardíaco (latidos/minuto) después de levantarse. \\(H_0 : \\theta = 15\\).\nPEGAR TABLA DE DIAPOSITIVA 29\n\\[\\begin{aligned}\nT_{c} & =\\sum R_{i}^{+} S\\left(y_{i}\\right)=14.5 \\\\\nE(T) & =\\frac{n(n+1)}{4}=39 \\\\\nV(T) & =\\frac{n(n+1)(2 n+1)}{24}=162.375 \\\\\nZ_{c} & =\\frac{T_{c}-E(T)}{\\sqrt{V(T)}}=-1.9226\n\\end{aligned}\\]\nSe rechaza \\(H_0\\) al 5% si \\(Z_{c}&gt;Z_{1-\\alpha}=1.645\\) Luego, no hay evidencia para rechazar \\(H_0\\)"
  },
  {
    "objectID": "pareadas.html#ejemplo-conteo-de-bacterias-en-muestras-de-leche",
    "href": "pareadas.html#ejemplo-conteo-de-bacterias-en-muestras-de-leche",
    "title": "4  Muestras pareadas",
    "section": "4.1 Ejemplo: Conteo de Bacterias en muestras de leche",
    "text": "4.1 Ejemplo: Conteo de Bacterias en muestras de leche\nDiez muestras de leche se dividen en dos porciones. Una porción se envía al laboratorio I y la otra al laboratorio II. Cada laboratorio hace recuentos bacterianos (miles de bacterias por ml). Los datos se muestran en la tabla a continuación. Con base en los datos puede concluirse que hay diferencias entre los laboratorios?. Puede emplearse una prueba t-student para muestras pareadas?.\nEjemplo: 10 muestras de leche. A cada una se le mide el contenido bacteriano (miles de bacterias por ml) en dos laboratorios. Hay diferencias entre los laboratorios?\nPEGAR TABLA DE DIAPOSITIVA 5"
  },
  {
    "objectID": "pareadas.html#prueba-t-pareada.-hipótesis",
    "href": "pareadas.html#prueba-t-pareada.-hipótesis",
    "title": "4  Muestras pareadas",
    "section": "4.2 Prueba T pareada. Hipótesis",
    "text": "4.2 Prueba T pareada. Hipótesis\n\nMuestra aleatoria pareada:\n\\(\\left(X_{1_{1}}, X_{2_{1}}\\right), \\ldots,\\left(X_{1_{n}}, X_{2_{n}}\\right) . D_{i}=\\left(X_{1_{i}}-X_{2_{i}}\\right), i=1, \\ldots, n.\\)\nMuestra aleatoria pareada observada:\n\\(\\left(x_{1_{1}}, x_{2_{1}}\\right), \\ldots,\\left(x_{1_{n}}, x_{2_{n}}\\right) \\cdot d_{i}=\\left(x_{1_{i}}-x_{2_{i}}\\right), i=1, \\ldots, n \\text {. }\\)\nMuestra aleatoria: \\(D_1,...D_n\\)\nHipótesis pueden plantearse como:\n\\[\\begin{array}{l}\nH_{0}: \\mu_{D}=0 \\\\\nH_{a}: \\mu_{D} \\neq 0 \\\\\n\\mu_{D}&gt;0 \\\\\n\\mu_{D}&lt;0\n\\end{array}\\]"
  },
  {
    "objectID": "pareadas.html#estadística-de-prueba-y-región-de-rechazo",
    "href": "pareadas.html#estadística-de-prueba-y-región-de-rechazo",
    "title": "4  Muestras pareadas",
    "section": "4.3 Estadística de Prueba y Región de Rechazo",
    "text": "4.3 Estadística de Prueba y Región de Rechazo\n\\[\\begin{aligned}\nT & =\\frac{\\bar{D}}{\\frac{S_{D}}{\\sqrt{n}}} \\sim \\text { t-student }_{n-1}, \\text { con } \\\\\n\\bar{D} & =\\frac{\\sum_{i=1}^{n} D_{i}}{n} \\text { y } S_{D}=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(D_{i}-\\bar{D}\\right)^{2}}{n-1} .} \\\\\n\\text { Sea } T_{c} & =\\frac{\\bar{d}}{\\frac{s_{d}}{n}} \\operatorname{con} \\bar{d}=\\frac{\\sum_{i=1}^{n} d_{i}}{n} \\text { y } s_{d}=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(d_{i}-\\bar{d}\\right)^{2}}{n-1}}\n\\end{aligned}\\]\nSe rechaza la hipótesis nula al nivel \\(\\alpha\\) si\n\\[\\begin{array}{l}\nT_{c}&lt;t_{\\frac{\\alpha}{2}} \\circ T_{c}&gt;t_{1-\\frac{\\alpha}{2}} \\\\\nT_{c}&lt;t_{\\alpha} \\\\\nT_{c}&gt;t_{1-\\alpha}\n\\end{array}\\]"
  },
  {
    "objectID": "pareadas.html#test-del-signo.-muestras-pareadas",
    "href": "pareadas.html#test-del-signo.-muestras-pareadas",
    "title": "4  Muestras pareadas",
    "section": "4.4 Test del signo. Muestras pareadas",
    "text": "4.4 Test del signo. Muestras pareadas\n\nPoblación: sea \\((X_1-X_2)\\) una variable aleatoria bivariada y \\(Y=\\left(X_{1}-X_{2}\\right)\\). Suponga que \\(Y\\) tiene mediada \\(\\theta, F_{Y}(\\theta)=\\frac{1}{2}\\) (si \\(Y\\) es continua) o \\(\\circ P(Y \\leq \\theta) \\geq \\frac{1}{2}\\) y \\(P(Y \\geq \\theta) \\geq \\frac{1}{2}\\) (si \\(Y\\) es discreta).\nMuestra: \\(Y_{1}, Y_{2}, \\ldots, Y_{n}\\) una muestra aleatoria de \\(Y\\).\nMuestra observada \\(Y_{1}, Y_{2}, \\ldots, Y_{n}\\) una muestra aleatoria observada de \\(Y\\).\nHIpótesis:\n\n\\[\\begin{aligned}\nH_{0}: \\theta & =0 \\\\\nH_{a}: \\theta & \\neq 0 \\\\\n\\theta & &gt;0 \\\\\n\\theta & &lt;0\n\\end{aligned}\\]\n\nEstadística de prueba y región de rechazo. \\(H_{a}: \\theta&gt;0\\)\n\n\\[S=\\sum_{i=1}^{n} \\psi_{i}\\] con,\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i}&gt;0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\nBajo \\(H_{0}, P\\left(\\psi_{i}=1\\right)=P\\left(Y_{i}&gt;0\\right)=\\frac{1}{2}\\) \\[\\Rightarrow S \\sim \\operatorname{Bin}\\left(n, \\frac{1}{2}\\right)\\] Se rechaza \\(H_0\\) si\n\\(S \\geq k\\), donde \\(k\\) es tal que \\(P(S \\geq k)=\\alpha\\), con \\(\\alpha\\) el nivel de significancia.\n\nEstadística de prueba y región de rechazo. \\(H_{a}: \\theta&lt;0\\)\n\n\\[S=\\sum_{i=1}^{n} \\psi_{i}\\] con,\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i}&gt;0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\nBajo \\(H_{0}, P\\left(Y_{i}=1\\right)=P\\left(Y_{i}&gt;0\\right)=\\frac{1}{2}\\)\n\\[\\Rightarrow S \\sim \\operatorname{Bin}\\left(n, \\frac{1}{2}\\right)\\] Se rechaza \\(H_0\\) si\n\\(S \\leq k\\), donde \\(k\\) es tal que \\(P(S \\leq k)=\\alpha\\) y \\(alpha\\) es el nivel de significancia.\n\nEstadística de prueba y región de rechazo. \\(H_{a}: \\theta \\neq 0\\)\n\n\\[S=\\sum_{i=1}^{n} \\psi_{i}\\]\ncon,\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i} \\geq 0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\nBajo \\(H_{0}, P\\left(Y_{i}=1\\right)=P\\left(Y_{i}&gt;0\\right)=\\frac{1}{2}\\)\n\\[\\Rightarrow S \\sim \\operatorname{Bin}\\left(n, \\frac{1}{2}\\right)\\]\nSe rechaza \\(H_0\\) si\n\\(S \\leq k_{1}\\), donde \\(k_{1}\\) es tal que \\(P\\left(S \\leq k_{1}\\right)=\\alpha / 2\\), o si \\(S \\geq k_{2}\\), donde \\(k_{2}\\) es tal que \\(P\\left(S \\geq k_{2}\\right)=\\alpha / 2\\), con \\(\\alpha\\) el nivel de significancia."
  },
  {
    "objectID": "pareadas.html#test-de-wilcoxon.-muestras-pareadas",
    "href": "pareadas.html#test-de-wilcoxon.-muestras-pareadas",
    "title": "4  Muestras pareadas",
    "section": "4.5 Test de Wilcoxon. Muestras pareadas",
    "text": "4.5 Test de Wilcoxon. Muestras pareadas\n\nPoblación: \\(\\left(X_{1}, X_{2}\\right)\\) variable aleatoria bivariada. \\(Y=\\left(X_{1}-X_{2}\\right)\\). Suponga que \\(Y\\) tiene distribución continua simétrica con mediana \\(\\theta\\).\nMuestra \\(Y_{1}, \\ldots, Y_{n}\\) una muestra aleatoria de \\(Y\\).\nHipótesis:\n\n\\[\\begin{aligned}\nH_{0}: \\theta & =0 \\\\\nH_{a}: \\theta & \\neq 0 \\\\\n\\theta & &gt;0 \\\\\n\\theta & &lt;0\n\\end{aligned}\\]\n\\(Y_{1}, Y_{2}, \\ldots, Y_{n}\\) una muestra aleatoria de \\(Y\\).\n\\[\\psi_{i}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } Y_{i} \\geq 0 \\\\\n0 & \\text { if } Y_{i}&lt;0\n\\end{array}\\right.\\]\n\\(R_{i}^{+}=\\) Posición que ocupa \\(\\left|Y_{i}\\right|\\) en la sucesión ordenada\n\\[|Y|_{(1)} \\leq|Y|_{(2)} \\leq \\ldots \\leq|Y|_{(n)}\\]\n\nEstadística de prueba:\n\n\\[\\begin{array}{l}\nT=\\sum_{i=1}^{n} R_{i}^{+} \\psi_{i} \\\\\nT_{c}=\\sum_{i=1}^{n} R_{i}^{+} \\psi_{i}\n\\end{array}\\]\nSe puede demostrar que\n\\[\\begin{array}{c}\nE(T)=\\frac{n(n+1)}{4} \\\\\nV(T)=\\frac{n(n+1)(2 n+1)}{24}\n\\end{array}\\]\n\nDistribución asintótica: Para \\(n\\) grande.\n\n\\[Z=\\frac{T-E(T)}{\\sqrt{V(T)}} \\sim N(0,1)\\] El valor calculado de la estadística de prueba es\n\\[Z_{c}=\\frac{T_{c}-E(T)}{\\sqrt{V(T)}}\\]\nRegión de rechazo. \\(H_a:\\theta&gt;0\\) Se rechaza \\(H_0\\) si \\(Z_{c}&gt;z_{1-\\alpha}\\).\nRegión de rechazo. \\(H_a:\\theta&lt;0\\) Se rechaza \\(H_0\\) si \\(Z_{c}&lt;z_{\\alpha}\\)\nRegión de rechazo. \\(H_a:\\theta \\neq 0\\) Se rechaza \\(H_0\\) si \\(Z_{c}&lt;z_{\\frac{\\alpha}{2}} \\circ Z_{c}&gt;z_{1-\\frac{\\alpha}{2}}\\)\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "muestrasind.html#prueba-clásica-asumiendo-normalidad-prueba-t-de-dos-muestras-independientes",
    "href": "muestrasind.html#prueba-clásica-asumiendo-normalidad-prueba-t-de-dos-muestras-independientes",
    "title": "5  Muestras independientes",
    "section": "5.1 Prueba clásica asumiendo normalidad Prueba T de dos muestras independientes",
    "text": "5.1 Prueba clásica asumiendo normalidad Prueba T de dos muestras independientes\n\nMuestra aleatoria \\(X_{1}, \\ldots, X_{n}, Y_{1}, \\ldots, Y_{m}\\)\nMuestra aleatoria observada \\(x_{1}, \\ldots, x_{n}, y_{1}, \\ldots, y_{m}\\)\nHipótesis\n\n\\[\\begin{aligned}\nH_{0}: \\mu_{1} & =\\mu_{2} \\\\\nH_{a}: \\mu_{1} & \\neq \\mu_{2} \\\\\n\\mu_{1} & &gt;\\mu_{2} \\\\\n\\mu_{1} & &lt;\\mu_{2}\n\\end{aligned}\\]\n\n5.1.1 Estadística de prueba y región de rechazo\n\\[\\begin{aligned}\nT & =\\frac{(\\bar{X}-\\bar{Y})}{S_{p} \\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\sim \\text { t-student }{ }_{n+m-2}, \\text { con } \\\\\nS_{p} & =\\sqrt{\\frac{(n-1) S_{x}^{2}+(m-1) S_{y}^{2}}{n+m-2}} . \\\\\nS_{\\text {Sea }} T_{c} & =\\frac{(\\bar{x}-\\bar{y})}{s_{p} \\sqrt{\\frac{1}{n}+\\frac{1}{m}}} \\operatorname{con} s_{p}=\\sqrt{\\frac{(n-1) s_{x}^{2}+(m-1) s_{y}^{2}}{n+m-2}}\n\\end{aligned}\\]\nSe rechaza la hipótesis nula al nivel \\(\\alpha\\) si\n\\[\\begin{array}{l}\nT_{c}&lt;t_{\\frac{\\alpha}{2}} \\circ T_{c}&gt;t_{1-\\frac{\\alpha}{2}} \\\\\nT_{c}&gt;t_{1-\\alpha} \\\\\nT_{c}&lt;t_{\\alpha}\n\\end{array}\\]"
  },
  {
    "objectID": "muestrasind.html#pruebas-no-paramétricas-para-dos-muestras-independientes",
    "href": "muestrasind.html#pruebas-no-paramétricas-para-dos-muestras-independientes",
    "title": "5  Muestras independientes",
    "section": "5.2 Pruebas no paramétricas para dos muestras independientes",
    "text": "5.2 Pruebas no paramétricas para dos muestras independientes\n\nLos datos provienen de distribuciones continuas\nSe asume independencia (dentro de muestras y entre muestras)\nLas pruebas de Wilcoxon y de Mann-Whitney para dos muestras son alternativas no paramétricas a la prueba T-student para dos muestras."
  },
  {
    "objectID": "muestrasind.html#pruebas-de-wilcoxon-y-de-mann-whitney",
    "href": "muestrasind.html#pruebas-de-wilcoxon-y-de-mann-whitney",
    "title": "5  Muestras independientes",
    "section": "5.3 Pruebas de Wilcoxon y de Mann-Whitney",
    "text": "5.3 Pruebas de Wilcoxon y de Mann-Whitney\n\nDos versiones del test, descritas comoU de Mann–Whitney y W de Wilcoxon, fueron independientemente desarrolladas por Mann y Whitney (1947) y Wilcoxon (1949).\nLa versión por Wilcoxon (1949) es usualmente llamada test W de Wilcoxon–Mann-Whitney.\nAunque se emplean diferentes ecuaciones y diferentes tablas de distribuciones exactas, las dos versiones producen resultados comparables.\n\n\n5.3.1 Ejemplo dieta de cabras\n\nTomado de McFarland, T and Yates, J. 2016. Introduction to non parametric statistics for the biological sciences using R. Springer. Una manada de 30 cabras fue dividida en dos grupos. La asignación al grupo 1 o al grupo 2 se basó en una selección aleatoria.\nGrupo 1: Control. Estas cabras recibieron alimentación regular durante el tratamiento.\nGrupo 2: Tratamiento. Estas cabras recibieron un suplemento mineral además de la alimentación regular. Respuesta: Un valor (índice) entre 40 y 100 dependiendo de las condiciones de la cabra.\nEjemplo: 15 cabras asignadas a un grupo control (dieta regular) y uno un grupo de tratamiento (dieta regular con adición de minerales). La respuesta es un índice con valores entre 40 y 100.\nPEGAR TABLA DE DIAPOSITIVA 8\n5.4 Test de Wilcoxon\n\n\n\n5.4.1 Supuesto poblacional e hipótesis\n\n\\(X_{1} \\ldots, X_{m}\\) muestra aleatoria de \\(X \\sim F_{X}(t)\\) continua\n\\(Y_{1} \\ldots, Y_{n}\\) muestra aleatoria de \\(Y \\sim F_{Y}(t)\\) continua\nIndependencia entre \\(X\\) y \\(Y\\)\nHipótesis\n\n\\[\\begin{array}{l}\nH_{0}: F_{Y}(t)=F_{X}(t) \\text { para todo } t \\\\\nH_{a}: F_{Y}(t)=F_{X}(t-\\Delta) \\text { para todo } t \\text { y algún } \\Delta \\neq 0\n\\end{array}\\]\nGibbons, J and Chakraborti, S. 2003. Nonparametrical Statistical Inference. Marcel Dekker.\n\n\n5.4.2 Normal\nPEGAR FIGURA DIAPOSITIVA 10\n\n\n5.4.3 Exponencial y exponencial desplazada\nPEGAR FIGURA DIAPOSITIVA 11\n\n\n5.4.4 Weibull y Weibull Desplazada\nPEGAR FIGURA DIAPOSITIVA 12\n∆ en (1) se llama parámetro de desplazo o efecto de tratamiento.\n\nLas hipótesis pueden plantearse como\n\n\\[\\begin{aligned}\nH_{0}: \\Delta & =0 \\\\\nH_{a}: \\Delta & \\neq 0 \\\\\n\\Delta & &gt;0 \\\\\n\\Delta & &lt;0\n\\end{aligned}\\]\n\nMuestra Combinada: sea \\(X_{1}, X_{2}, \\ldots, X_{m}, X_{m+1}, \\ldots, X_{N}\\), \\(N=m+n\\) la muestra combinada. \\(X^{\\prime} s\\) \\(X_{m+1}, \\ldots, X_{N}\\) las observaciones de las \\(Y^{\\prime} s\\).\n\n\n\n5.4.5 Estadística de prueba\n\nSe ordena la muestra combinada de \\(N=m+n\\) variables.\nLlámese \\(W=\\) suma de los rangos de las \\(Y^{\\prime} s\\) en la muestra combinada. Sean \\(S_{1}, S_{2}, \\ldots, S_{n}\\). (W puede definirse como la suma de los rangos de las \\(X^{\\prime} s\\))\n\n\\[\\begin{aligned}\nW & =\\sum_{i=1}^{n} S_{j} \\\\\n\\mathbb{E}(W) & =\\frac{n(N+1)}{2}=\\frac{n(m+n+1)}{2} \\\\\n\\mathbb{V}(W) & =\\frac{n m(N+1)}{12}=\\frac{n m(m+n+1)}{12}\n\\end{aligned}\\]\n\n\n5.4.6 Distribución asintótica\n\\[Z=\\frac{W-\\mathbb{E}(W)}{\\sqrt{\\mathbb{V}(W)}} \\sim N(0,1)\\]\nSe rechaza la hipótesis nula \\(\\alpha\\) si\n\\[\\begin{array}{l}\nZ_{c}&lt;z_{\\frac{\\alpha}{2}} \\circ Z_{c}&gt;z_{1-\\frac{\\alpha}{2}} \\\\\nZ_{c}&gt;z_{1-\\alpha} \\\\\nZ_{c}&lt;z_{\\alpha}\n\\end{array}\\]\n\n\n5.4.7 Distribución exacta\nSuponga \\(m =3\\) valores de \\(X\\) y \\(n = 2\\) valores de \\(Y\\). Los rangos van entre 1 y 5. Ver Hollander and Wolfe página 108.\nPEGAR TABLA DIAPOSITIVA 16\n\n\n5.4.8 Empates\n\nCaso de distribución exacta: Se da a las observaciones empatadas el promedio de los correspondientes rangos, se calcula W y se usa la misma tabla\nSi se emplea la distribución asintótica cambia la varianza\n\n\\[V(W)=\\frac{m n(N+1)}{12}-\\left(\\frac{m n}{12 N(N-1)} \\sum_{j=1}^{g}\\left(t_{j}-1\\right) t_{j}\\left(t_{j}+1\\right)\\right)\\]\ncon \\(g\\) el número de grupos con empates y \\(t_{j}\\) el número de observaciones empatadas en el j-ésimo grupo."
  },
  {
    "objectID": "muestrasind.html#test-de-mann-whitney",
    "href": "muestrasind.html#test-de-mann-whitney",
    "title": "5  Muestras independientes",
    "section": "5.5 Test de Mann-Whitney",
    "text": "5.5 Test de Mann-Whitney\n\n5.5.1 Hipótesis\nComo en el test de Wilcoxon, las hip´otesis pueden plantearse como\n\\[\\begin{aligned} H_{0}: \\Delta & =0 \\\\ H_{a}: \\Delta & \\neq 0 \\\\ \\Delta & &gt;0 \\\\ \\Delta & &lt;0\\end{aligned}\\]\n\n\n5.5.2 Estadística de prueba\nSea\n\\[\\phi(a)=\\left\\{\\begin{array}{ll}\n1 & \\text { if } a&gt;0 \\\\\n0 & \\text { if } a&lt;0\n\\end{array}\\right.\\]\nLa estadística de Mann-Whitney se define como:\n\\[\\begin{aligned}\nU & =\\sum_{i=1}^{n} \\sum_{j=1}^{m} \\phi\\left(Y_{i}-X_{j}\\right) \\\\\n& =\\#\\left(Y_{i}-X_{j}\\right)&gt;0\n\\end{aligned}\\]\nSi no hay empates\n\\[U=W-\\frac{n(n+1)}{2}\\]\n\n\n5.5.3 Distribución asintótica\n\\[\\begin{aligned} U & =W-\\frac{n(n+1)}{2} \\\\ \\mathbb{E}(U) & =\\mathbb{E}(W)-\\frac{n(n+1)}{2} \\\\ & =\\frac{n(N+1)}{2}-\\frac{n(n+1)}{2} \\\\ & =\\frac{n(N+1)-n(n+1)}{2} \\\\ & =\\frac{n(N+1-(n+1))}{2} \\\\ & =\\frac{n(n+m+1)-(n+1)}{2} \\\\ & =\\frac{n m}{2}\\end{aligned}\\]\n\\[\\begin{aligned}\nU & =W-\\frac{n(n+1)}{2} \\\\\n\\mathbb{V}(U) & =\\mathbb{V}(W)-\\mathbb{V}\\left(\\frac{n(n+1)}{2}\\right) \\\\\n& =\\frac{n m(m+n+1)}{12} \\\\\nZ & =\\frac{U-\\mathbb{E}(U)}{\\sqrt{\\mathbb{V}(U)}} \\sim N(0,1)\n\\end{aligned}\\]\nSe rechaza la hipótesis nula al nivel \\(\\alpha\\) si\n\\[\\begin{array}{l}\nZ_{c}&lt;z_{\\frac{\\alpha}{2}} \\circ Z_{c}&gt;z_{1-\\frac{\\alpha}{2}} \\\\\nZ_{c}&gt;z_{1-\\alpha} \\\\\nZ_{c}&lt;z_{\\alpha}\n\\end{array}\\]\n\n\n5.5.4 Ejemplo R\n\nVer código R: Wilcoxon y Mann-Whitney.R\nUsar la base de datos Goats.df correspondiente a los datos de las cabras arriba descritos.\nHacer la prueba T de dos muestras y el test de Wilcoxon (R lo llama Wilcoxon pero calcula la estadística de Mann-Whitney).\n\nREVISAR SI EL CÓDIGO R VA EN ESTA PARTE\nFALTAN DIAPOSITIVAS 23-25 CREO QUE SALE DEL CÓDIGO\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "anova.html#test-de-kruskal-wallis",
    "href": "anova.html#test-de-kruskal-wallis",
    "title": "6  Anova una vía",
    "section": "6.1 Test de Kruskal-Wallis",
    "text": "6.1 Test de Kruskal-Wallis\n\n6.1.1 Ejemplo de Hipertensión\nEjemplo: Estudio de hipertensión. 25 pacientes. Tratamientos: Control, dieta baja en sal, dieta sin sal, dosis 1 de un fármaco y dosis 2 fármaco. Respuesta: Las presiones arteriales sistólicas al final del tratamiento.\nPEGAR TABLA DIAPOSITIVA 3\n\n\n6.1.2 Diseños Experimentales\n\nDiseño experimental: se emplea cuando se buscan relaciones entre una variable cuantitativa dependiente (variable respuesta) y una o varias variables cualitativas independientes.\nDiseño completamente aleatorio: los tratamientos o niveles de un factor son aplicados aleatoriamente a un conjunto homogéneo de unidades experimentales.\nVarias poblaciones: Cada tratamiento representa una población de la cual se obtiene una muestra.\nNormalidad y homogeneidad: Se asume que las distribuciones de la respuesta para cada nivel del factor son normales con varianzas iguales.\n\n\n\n6.1.3 Conceptos Fundamentales\n\nFactor:Variable Cualitativa.\nRespuesta:Variable Cuantitativa.\nTratamientos:Niveles del factor.\nUnidad experimental:Individuo sobre el cuál se hace la medición de las variables.\nUnidad muestral:Varias observaciones dentro de la misma unidad experimental.\nReplicas:Número de unidades experimentales por tratamiento.\nError experimental:Unidades experimentales homogéneas tienen respuestas distintas.\nEstructura de tratamientos:Factores involucrados en el análisis (una vía, dos vías, factorial).\nEstructura de diseño: Cómo se asignan las unidades experimentales a los tratamientos (DCA, DBA, Anidado, Parcelas Divididas).\n\n\n\n6.1.4 Evaluación de Supuestos\n\nNormalidad Test de Kolmogorov-Smirnov (Lilliefors) Shapiro-Wilk, Anderson Darling, Cramer von Mises, Jarque-Vera, DAgostino, etc, con las observaciones de cada tratamiento (varias pruebas) o con los residuales del modelo ANOVA.\nHomocedasticidadBartlett, Levene, Cochran, etc, con base en las muestras de los tratamientos.\n\n\n\n6.1.5 Tabla de Datos Diseño a una Vía\nTable: Arreglo de datosen un dise~no experimental con un solo factor de \\(k\\) tratamientos (niveles del factor) y \\(n\\) datos por tratamiento.\nPEGAR TABLA DIAPOSITIVA 8\n\n\n6.1.6 Muestra Aleatoria e Hipótesis\nTable: Arreglo con la muestra aleatoriade un dise~no experimental con un solo factor de \\(k\\) tratamientos (niveles del factor) y \\(n\\) datos por tratamiento.\nPEGAR TABLA DIAPOSITIVA 9\n\\[\\begin{array}{l}H_{0}: \\mu_{1}=\\mu_{2}=\\ldots=\\mu_{k} \\\\ H_{a}: \\mu_{i} \\neq \\mu_{j}\\end{array}\\] ### Hipótesis\nEn términos poblacionales\n\\[\\begin{array}{l}\nH_{0}: Y_{i j}=\\mu+\\epsilon_{i j}, i=1, \\ldots, n ; j=1, \\ldots, k \\\\\nH_{a}: Y_{i j}=\\mu+\\lambda_{j}+\\epsilon_{i j}\n\\end{array}\\]\nEn términos muestrales se tendría\n\\[\\begin{aligned}\nY_{i j} & =\\overline{\\bar{Y}}+e_{i j} \\\\\n& =\\overline{\\bar{Y}}+\\left(Y_{i j}-\\bar{Y}_{j}\\right) \\\\\nY_{i j} & =\\overline{\\bar{Y}}+\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+e_{i j} \\\\\n& =\\overline{\\bar{Y}}+\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right) \\\\\n\\left(Y_{i j}-\\overline{\\bar{Y}}\\right) & =\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right)\n\\end{aligned}\\]\n\n\n6.1.7 Descomposición de la Variabilidad\n\\[\\begin{aligned}\\left(Y_{i j}-\\overline{\\bar{Y}}\\right) & =\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right) \\\\ \\left(Y_{i j}-\\overline{\\bar{Y}}\\right)^{2} & =\\left[\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right)\\right]^{2} \\\\ & =\\left[\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)^{2}+2\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)\\left(Y_{i j}-\\bar{Y}_{j}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right)^{2}\\right] \\\\ \\sum_{i=1}^{n_{j}} \\sum_{j=1}^{k}\\left(Y_{i j}-\\overline{\\bar{Y}}\\right)^{2} & =\\sum_{i=1}^{n_{j}} \\sum_{j=1}^{k}\\left[\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)^{2}+2\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)\\left(Y_{i j}-\\bar{Y}_{j}\\right)+\\left(Y_{i j}-\\bar{Y}_{j}\\right)^{2}\\right] \\\\ & =\\sum_{i=1}^{n_{j}} \\sum_{j=1}^{k}\\left(\\bar{Y}_{j}-\\overline{\\bar{Y}}\\right)^{2}+\\sum_{i=1}^{n_{j}} \\sum_{j=1}^{k}\\left(Y_{i j}-\\bar{Y}_{j}\\right)^{2} \\\\ S C T & =S C T R+S C E\\end{aligned}\\]\n\n\n6.1.8 Tabla de Análisis de Varianza\nTable: Tabla de descomposición de la varianza para un diseño a una vía, basada en la muestra aleatoria \\(Y_{i j}, i=1, \\ldots, n, j=1, \\ldots, k\\)\nRegión de Rechazo: Se rechaza \\(H_0\\) con un nivel de significancia \\(\\alpha\\) si \\(F&gt;F_{(k-1),(N-k), 1-\\alpha}\\)\n\n\n6.1.9 Tabla de Análisis de Varianza. Datos Observados\nTable: Cálculo de la estadística F. Datos observados: sctr, sce, cmtr, cme, son valores reales (observaciones).\nPEGAR TABLA DIAPOSITIVA 13\nSe rechaza \\(H_0\\) con un nivel de significancia \\(\\alpha\\) si \\(F_{c}&gt;F_{(k-1),(N-k), 1-\\alpha} . F_{c}\\) es el valor calculado de \\(F\\) con base en \\(y_{i j}, i=1, \\ldots, n ; j=1, \\ldots, k\\)\n\n\n6.1.10 Homogeneidad de Varianzas. Prueba de Hartley\nRequiere el mismo \\(n\\)\n\n\\(H_{0}: \\sigma_{1}^{2}=\\ldots=\\sigma_{k}^{2}\\)\n\\(H a: \\sigma_{i}^{2} \\neq \\sigma_{j}^{2}, i, j=1, \\ldots, k, i \\neq j\\)\n\\(F_{\\max }=\\frac{S_{\\max }^{2}}{S_{\\min }^{2}}\\)\nSe rechaza \\(H_0\\) si \\(F_{\\max }&gt;F_{1-\\alpha, k,(n-1)}\\)\n\n\n\n6.1.11 Homogeneidad de Varianzas. Prueba de Bartlett\nPermite diferentes \\(n\\)\n\n\\(H_{0}: \\sigma_{1}^{2}=\\ldots=\\sigma_{k}^{2}\\)\n\\(H a: \\sigma_{i}^{2} \\neq \\sigma_{j}^{2}, i, j=1, \\ldots, k, i \\neq j\\) \\(C=\\left(\\sum_{j=1}^{k}(n-1) \\ln \\bar{S}^{2}-\\sum_{j=1}^{k}(n-1) \\ln S_{j}^{2}\\right), \\mathrm{con}\\) \\(\\\\\\bar{S}^{2}=\\sum_{j=1}^{k}\\) \\(\\frac{S_{j}^{2}}{k}\\)\nSe rechaza \\(H_0\\) si \\(C_{\\max }&gt;\\chi_{\\alpha-1, k}^{2}\\)\n\n\n\n6.1.12 Homogeneidad de Varianzas. Prueba de Levene\n\\(Z_{i j}=\\left|Y_{i j}-\\tilde{Y}_{j}\\right|, \\tilde{Y}_{j}\\) es la mediana de \\(j\\)-ésimo grupo, \\(j=1, \\ldots, k\\)\n\\[\\begin{array}{l}\nL=\\frac{\\sum_{j=1}^{k} n_{j}\\left(\\bar{Z}_{j}-\\bar{Z}\\right)^{2} /(k-1)}{\\sum_{j=1}^{k} \\sum_{i=1}^{n_{i}}\\left(Z_{i j}-\\bar{Z}_{j}\\right)^{2} /\\left(n_{k}-k\\right)} \\\\\nn_{k}=\\sum_{j=1}^{k} n_{j}\n\\end{array}\\]\nSe rechaza \\(H_0\\) si \\(L&gt;F_{(1-\\alpha),(k-1),\\left(n_{k}-1\\right)}\\)\n\n\n6.1.13 Pruebas de Comparación Múltiple\n\nSe utilizan cuando se rechaza la hipótesis nula, para establecer entre cuales de los tratamientos hay diferencias significativas.\nAlgunas de las pruebas comúnmente usadas son LSD (Least Significant Difference), Tukey, Duncan, Schffe y Bonferroni, Dunnet,…\n\n\n\n6.1.14 Diferencia Mínima Significativa (LSD)\n\n\\(S_{R}^{2}:\\): Varianza residual del ANOVA\n\\(\\bar{y}_{j}\\) y \\(n_j\\): Media y el tamaño de muestra del tratamiento \\(j, j=1, \\ldots, k\\)\n\\(t_{\\frac{\\alpha}{2}}\\): Cuantila de una distribución T-student con \\((n-k)\\) g.l\n\\(\\mathrm{LSD}=t_{\\frac{\\alpha}{2}, N-k} \\sqrt{S_{R}^{2}\\left(\\frac{1}{n_{i}}+\\frac{1}{n_{j}}\\right)}\\)\nSi \\(\\left|\\bar{y}_{i}-\\bar{y}_{j}\\right|&gt;\\mathrm{LSD}\\), se rechaza la hipótesis \\(\\mathrm{H}_{0}: \\mu_{i}=\\mu_{j}\\) al nivel de significancia \\(\\alpha\\).\n\n\n\n6.1.15 Prueba de Tukey\n\n\\(H S D=q_{\\alpha}(p, v) \\sqrt{\\frac{C M E}{n}}\\)\n\\(q_{\\alpha}(p, v)\\) es el valor crítico del rango estudentizado de Tukey (valor de tabla) con \\(p\\) igual al número de tratamientos y v los grados de libertad asociados al \\(CME\\).\nOrdene las \\(p\\) medias muestrales. Si la diferencia entre dos medias muestrales es mayor del valor calculado de \\(HSD\\) entonces la conclusión es que hay diferencias entre las medias (poblacionales) de los tratamientos.\nRequiere igual número de replicas por tratamientos (hay un ajuste para tamaños de muestra distintos). https://www.real-statistics.com/statistics-tables/studentized-range-q-table/\n\n\n\n6.1.16 Test de Kruskal-Wallis\nSupuestos: \\(k\\) muestras independientes de distribuciones continuas Hipótesis: La mediana de las \\(k\\) distribuciones es igual\n\n\\(H_{0}: \\theta_{1}=\\theta_{2}=\\ldots=\\theta_{k}\\)\n\\(H_{a}: \\theta_{i} \\neq \\theta_{j}\\), para algún \\(i \\neq j\\)\n\nEstadística de Prueba\nPEGAR “TABLA” DIAPOSITIVA 20\n\\(R_{i j}=\\) Rango de la \\(i\\)-ésima variable del \\(j\\)-ésimo grupo en la muestra combinada \\(Y_{11}, \\ldots, Y_{n_{k} k} . n_{j}\\) es el tamaño de muestra en el grupo \\(j, j=1, \\ldots, k\\)\n\n\n6.1.17 Distribución\n\n6.1.17.1 Revisión de los Supuestos:\n\nLos textos no están de acuerdo respecto al supuesto distribucional. Algunos mencionan que solo se requieren variables ordinales (Conover, Sheskin) y otros que estas deben ser continuas (Hollander and Wolfe, Gibbons, Klake and Mckean)\nKruskal, W. and Wallis,A. 1952. Use of Ranks in One-Criterion Variance Analysis. Use of Ranks in One-Criterion Variance Analysis, JASA, 47:260, 583-621.El test puede hacerse con variables con escala ordinal y usar la distribución exacta. Si se usa la aproximación \\(\\chi_{k-1}^{2}\\) se requiere continuidad.\n\n\n\n\n6.1.18 Hipótesis Sobre \\(k\\) Muestras Independientes\n\\[\\begin{array}{l}\nH=\\frac{12}{N(N+1)} \\sum_{j=1}^{k} n_{j}\\left(\\bar{R}_{j}-\\frac{N+1}{2}\\right)^{2} \\\\\nH=\\frac{12}{N(N+1)}\\left(\\sum_{j=1}^{k} \\frac{R_{j}^{2}}{n_{j}}\\right)-3(N+1), \\quad \\operatorname{con} N=\\sum_{j=1}^{k} n_{j}\n\\end{array}\\]\nRegión de Rechazo:\n\nSi \\(k \\leq 5\\) y \\(n_{j}\\) son pequeños, se usa la distribución exacta (ver tabla A.12 Hollander and Wolfe y 13.13 de Zar).\nSi las muestras son grandes, bajo \\(H_{0}, H \\sim \\chi_{(k-1)}^{2}\\). Se rechaza \\(H_0\\), al nivel \\(\\alpha\\), si \\(H_{c}&gt;\\chi_{1-\\alpha, k-1}^{2}\\)\n\n\n\n6.1.19 Hipótesis Sobre \\(k\\) Muestras Independientes\nEjemplo: Se tiene información tomada de forma aleatoria, sobre la duración en horas de tubos de magnetrón (componentes de los hornos microondas).\n\nlibrary(ggpubr)\n\n\n\ndtubos=c(36, 49, 71, 48, 33, 31, 5, 60, 140, 67,\n          2, 59, 53,  5, 42)\nrank(dtubos)\n\n [1]  6.0  9.0 14.0  8.0  5.0  4.0  2.5 12.0 15.0 13.0  1.0 11.0 10.0  2.5  7.0\n\ngrupo=c(rep(seq(1,3), 5))\ncbind(dtubos, grupo)\n\n      dtubos grupo\n [1,]     36     1\n [2,]     49     2\n [3,]     71     3\n [4,]     48     1\n [5,]     33     2\n [6,]     31     3\n [7,]      5     1\n [8,]     60     2\n [9,]    140     3\n[10,]     67     1\n[11,]      2     2\n[12,]     59     3\n[13,]     53     1\n[14,]      5     2\n[15,]     42     3\n\ndatos2=as.data.frame(cbind(grupo, dtubos))\nggboxplot(datos2, x = \"grupo\", y = \"dtubos\", col=rgb(0,0.5,1), ylab = \"Duración Tubos Magnetrón\", xlab = \"Marca\")\n\n\n\n\n\nggline(datos2, x = \"grupo\", y = \"dtubos\", \n       add = c(\"mean_se\", \"jitter\"), \n       ylab = \"Duraci?n Tubos Magnetrón\", xlab = \"Grupo\")\n\n\n\n\n\nlibrary(dunn.test)\nkruskal.test(dtubos ~ grupo, data = datos2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  dtubos by grupo\nKruskal-Wallis chi-squared = 2.3191, df = 2, p-value = 0.3136\n\n\n\ndunn.test(dtubos, g=grupo, method=\"bonferroni\", kw=TRUE, label=TRUE, alpha=0.05)\n\n  Kruskal-Wallis rank sum test\n\ndata: dtubos and grupo\nKruskal-Wallis chi-squared = 2.3191, df = 2, p-value = 0.31\n\n                         Comparison of dtubos by grupo                         \n                                 (Bonferroni)                                  \nCol Mean-|\nRow Mean |          1          2\n---------+----------------------\n       2 |   0.707738\n         |     0.7187\n         |\n       3 |  -0.813899  -1.521638\n         |     0.6236     0.1921\n\nalpha = 0.05\nReject Ho if p &lt;= alpha/2\n\n\nAGREGAR TABLA DIAPOSITIVA 22\n\\[H=\\frac{12}{(15)(16)}\\left(\\sum_{j=1}^{3} \\frac{R_{j}^{2}}{n_{j}}\\right)-3(16)\\]\n\\[H_{c}=\\frac{12}{(15)(16)}\\left[\\frac{39.5^{2}}{5}+\\frac{29.5^{2}}{5}+\\frac{51^{2}}{5}\\right]-3(16)=2.315\\] Con, \\(\\alpha=5 \\%, \\chi_{2,0.95}^{2}=5.99\\)\n\nNo hay evidencia para rechazar \\(H_0\\). No hay evidencia para rechazar la hipótesis de igualdad de medianas de las 3 distribuciones es la misma.\n\n\n\n6.1.20 Tratamiento de Empates\n\\[H_{\\text {corr }}=\\frac{H}{1-C}, \\quad C=\\frac{\\sum_{i=1}^{m}\\left(t_{i}^{3}-t_{i}\\right)}{N^{3}-N}, H_{\\text {corr }}=2.3191\\] \\(t_i\\): Número de empates en el \\(i\\)-ésimo grupo de empates\n\\(m\\): Número de grupos con rangos empatados\n\n\n6.1.21 Comparaciones Múltiples. Dunn (ver PMCMR)\n\nNo debe usarse test de Wilcoxon de dos muestras porque aumenta la probabilidad de error tipo I.\nCalcule \\(R_{j}, j=1, \\ldots, k\\) Suma de los rangos de cada grupo.\nSe rechaza la hipótesis de que las poblaciones \\(i,j\\) son iguales si\n\n\\[\\left|\\frac{R_{j}}{n_{j}}-\\frac{R_{i}}{n_{i}}\\right|&gt;Z_{1-\\frac{\\alpha}{2}} \\sqrt{\\frac{N(N+1)}{12}\\left(\\frac{1}{n_{j}}+\\frac{1}{n_{i}}\\right)} i \\neq j, i, j=1,2 \\ldots, k\\]\nSi hay empates\n\\[\\begin{aligned}\n\\left|\\frac{R_{j}}{n_{j}}-\\frac{R_{i}}{n_{i}}\\right| & &gt;Z_{1-\\frac{\\alpha}{2}} \\sqrt{\\left(\\frac{N(N+1)}{12}-B\\right)\\left(\\frac{1}{n_{j}}+\\frac{1}{n_{i}}\\right)} \\\\\nB & =\\frac{\\sum_{i=1}^{m}\\left(t_{i}^{3}-t_{i}\\right)}{12(N-1)}\n\\end{aligned}\\]\n\n\n6.1.22 Ejercicio 1\nSe mide \\(X\\): Número de de moscas por metro cúbico de follaje. Se tienen tres tratamientos (hierba, arbustos y árboles). Pruebe la hipótesis de que la abundancia en las tres capas es la misma, usando el test de Kruskal-Wallis.\nPEGAR TABLA DIAPOSITIVA 27\n\n\n6.1.23 Ejercicio 2\n8 contenedores de agua se toman en cada uno de 4 estanques. Se mide el pH en cada muestra. Pruebe la hipÓtesis de que el pH en todos los estanques es el mismo, usando el test de Kruskal-Wallis.\nPEGAR TABLA DIAPOSITIVA 28\n\n1 + 1\n\n[1] 2"
  }
]